\chapter{Finding Similar Items}\label{sec:finding-similar-items}

We show how the problem of finding textually similar documents can be turned into such a set problem by the technique known as ``shingling''. We introduce a technique called ``minhashing'', which compresses large sets in such a way that we can still deduce the similarity of the underlying sets from their compressed versions. Another important problem that arises when we search for similar items of any kind is the problem of ``nearest neighbor search''. We show how this problem can be solved by a technique called ``Locality Sensitive Hashing''. 

\section{Application of Near-Neighbor Search}\label{sec:application-of-near-neighbor-search}

We shall focus initially on a particular notion of ``similarity'': the similarity of sets by looking at the relative size of their intersection. This notion is called \textit{Jaccard similarity}:
\begin{equation*}
    \text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation*}
where $A$ and $B$ are sets. The Jaccard similarity is a number between 0 and 1, where 0 means that the two sets are disjoint, and 1 means that they are equal.

\subsection{Similarity of Documents}\label{subsec:similarity-of-documents}

We can use the Jaccard similarity to measure the similarity of documents. We can represent a document as a set of words that occur in it. We can then measure the similarity of two documents by measuring the similarity of the sets of words that occur in them.

In the base case, where the two documents are exact duplicates, we can compare them character by character. However in many applications, the document are not identical, but they share a lot of part of text.

\begin{itemize}
    \item Plagiarism: finding plagiarized documents tests our ability to find textually similar documents.
    \item Mirror Pages: a mirror page is a page that is identical to another page, except for some minor changes, such as the replacement of the author's name or the addition of a few words.
    \item Articles from the Same Source: we may want to find articles from the same source, such as the New York Times.
\end{itemize}

\subsection{Collaborative Filtering as a Similar-Sets Problem}\label{subsec:collaborative-filtering-as-a-similar-sets-problem}

Another class of applications where similarity of sets is very important is called \textit{collaborative filtering}, a process where we raccomend to users items that other similar users have liked. 
\\
\\
\noindent \textbf{On-line Purchases} 

\noindent We can use collaborative filtering to recommend products to users based on the products that other similar users have purchased. We can say that two users are similar if their sets of purchased products have a high Jaccard similarity. 

Collaborative filtering requires us to find the nearest neighbors of a given user. However, by combining the similarity-finding with clustering, we are able to find the nearest neighbors of a user very quickly.
\\
\\
\noindent \textbf{Movie Ratings}

\noindent We can see movies as similar if they were rented or rated highly by many of the same customers, and see customers as similar if they rented or rated highly many of the same movies. 

When out data consists of rating rather then binary decisions, we cannot use the sets of items. Some alternative approaches are:

\begin{enumerate}
    \item Ignore low-rated customer/movie pairs.
    \item Use a threshold to convert ratings to binary decisions.
    \item If ratins are on a scale of 1 to 5, we can put in a multi-set the number of times each movie was rated. In order to compute Jaccard similarity of a multi-set, we can use the following formula:
    \begin{equation*}
        \text{Jaccard}(A, B) = \frac{\sum_{i=1}^n \min(A_i, B_i)}{\sum_{i=1}^n \max(A_i, B_i)}
    \end{equation*}
\end{enumerate}


\section{Shingling of Documents}\label{sec:shingling-of-documents}

The most effective way to represent documents as sets is to construct from the document the set of short string of characters that appear within it. These strings are called \textit{shingles}. 

A \textit{k}-shingle of a document is simply a sequence of $k$ characters that appears in it. For example, suppose that $D$ is the document containing the string ``\texttt{abcdadb}'' and that $k = 2$. Then the set of 2-shingles for $D$ is $\{ \texttt{ab}, \texttt{bc}, \texttt{cd}, \texttt{da}, \texttt{db} \}$. 

\subsection{Choosing the Shingle Size}\label{subsec:choosing-the-shingle-size}

We can pick $k$ to be any constant we like. But, if $k$ is too small, then the Jaccard similarity between two documents will be very high, even if the documents are completely different. On the other hand, if $k$ is too large, then we will miss the similarity between documents that share a lot of common text.

The important thing to remember is that, $k$ should be picked large enough that the probability of a given shingle appearning in any given document is low. 

A good rule of thumb is to immagine that there are only 20 characters and estimate the number of $k$-shingles as $20^k$. For example, for an email we can choose $k = 5$, since $20^5 = 3.2$ million and an email is unlikely to contain more than a few million characters. For a web page, we can choose $k = 9$, since $20^9 = 5.4$ trillion and a web page is unlikely to contain more than a few trillion characters.

\subsection{Hashing Shingles}\label{subsec:hashing-shingles}

Instead of using substring directly as shingles, we can pick a hash function $h$ and use $h(S)$ as the shingle for any string $S$. For instance, we can pick $k = 9$ and use the hash function $h(S) = \{0, 1, \dots, 2^{32} - 1\}$, we need to store only 4 bytes for each shingle. 

Notice that we can differentiate documents better if we use 9-shingles and hash them down to four bytes than to use 4-shingles, even though the space used to represent a shingle is the same.

\subsection{Shingles Built from Words}\label{subsec:shingles-built-from-words}

An alternative form of shingle has proved effective for the problem of identifying similar news articles, as mentioned in Section \ref{subsec:similarity-of-documents}. The exploitable distinction for this problem is that the news articles are written in a rather different style than are other elements that typically appear on the page with the article. However, for the problem of finding similar news articles, it was found that defining a shingle to be a stop word followed by the next two words, regardless of whether or not they were stop words, formed a useful set of shingles. 

\section{Similarity-Preserving Summaries of Sets}\label{sec:similarity-preserving-summaries-of-sets}

Set of shingles are large. Even if we hash them to four bytes each, the set needed to store a set is still large. The goal of this section is to replace large sets by much smaller representation called ``signatures'' that preserve the similarity of the sets.

The important property we need for signature is that we can compare the signature of two sets and get a good estimate of their Jaccard similarity. It is not possible that the signature of two sets are identical but their Jaccard similarity is low.

\subsection{Matrix Representation of Sets}\label{subsec:matrix-representation-of-sets}

Before explaining how it is possible create a small signature from a large set, we need to introduce a matrix representation of sets. The columns of the matrix correspond to the sets, and the row correspond to the elements of the universal sets from which elements of the sets are drawn.
Given a row $i$ and a column $j$, the entry $M_{ij}$ is 1 if the element $i$ is in the set $j$ and 0 otherwise.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
  \hline
  \textit{Element} & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  \textit{a} & 1 & 0 & 0 & 1  \\
  \textit{b} & 0 & 0 & 1 & 0  \\
  \textit{c} & 0 & 1 & 0 & 1  \\
  \textit{d} & 1 & 0 & 1 & 1  \\
  \textit{e} & 0 & 0 & 1 & 0  \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\caption{Matrix representation of sets}
\label{fig:matrix-representation-of-sets}
\end{figure}

It is important remember that the characteristic matrix is unlikely to be the way the data is stored in practice. The matrix representation is just a convenient way to think about sets.

\subsection{Minhashing}\label{subsec:minhashing}

The signature we deisre to construct for sets are composed of the results of a large number of calulation each of which is a ``minhash`` of the characteristic matrix. 

To \textit{minhash} a set represented by a column of the characteristic matrix, we first randomly permute the rows of the matrix. Then, the minhash value of the set is the number of the first row in which the set has a 1. 

\begin{equation*}
    \begin{split}
        h(S) &= \min_{i \in S} \boldsymbol{\pi}(i)\\
        \text{where } \boldsymbol{\pi}(x) &= \alpha x + \beta \text{ mod } n\\ 
        \text{and } \alpha, \beta &\text{ are random integers,}\\
        n & \text{ is number of rows in the matrix}
    \end{split}
\end{equation*}

\subsection{Minhashing and Jaccard Similarity}\label{subsec:minhashing-and-jaccard-similarity}

The minhashing technique has the property that the probability that the minhash of two sets is the same is equal to the Jaccard similarity of the sets.

To see why, we need to picture the columns for those two sets. If we restrict our attention to the column $S_1$ and $S_2$, then rows can divided into three categories:

\begin{enumerate}
    \item $X$ rows have 1 in both $S_1$ and $S_2$. 
    \item $Y$ rows have 1 in only one of $S_1$ and $S_2$ and 0 in the other.
    \item $Z$ rows have 0 in both $S_1$ and $S_2$.
\end{enumerate}

Remembering that the matrix is sparse, most rows are in category 3 ($Z$). However, it is the ratio between $X$ and $Y$ that determines the $\text{Jaccard}(S_1, S_2)$ and the $\mathbb{P}(h(S_1) = h(S_2))$.

Let $x \in X$ and $y \in Y$, then:

\begin{equation*}
    \begin{split}
        \text{Jaccard}(S_1, S_2) &= x / (x + y)\\
        \text{where } x &= |S_1 \cap S_2|\\
        x + y & = |S_1 \cup S_2|
    \end{split}
\end{equation*}

Consider the probability, immagine the rows permuted randomly and proceed from top. The $\mathbb{P}(\text{we meet a row in } X \text{ before we meet a row in } Y) = x / (x + y)$. If the first row we meet is in $X$, then $h(S_1) = h(S_2)$, but if the first row we meet is in $Y$, then $h(S_1) \neq h(S_2)$. We can conclude that:

\begin{equation*}
    \begin{split}
        \mathbb{P}(h(S_1) = h(S_2)) &= \mathbb{P}(\text{we meet a row in } X \text{ before we meet a row in } Y)\\
        &= x / (x + y)\\
        &= \text{Jaccard}(S_1, S_2)
    \end{split}
\end{equation*}

\subsection{Minhash Signature}\label{subsec:minhash-signature}

Let $M$ our characteristic matrix. To represent sets, we pick at random some number $n$ of premutations of the rows of $M$. Call the minhash functions determined by these permutations $h_1, h_2, \dots, h_n$. From the column representing set $S$, construct the \textit{minhash signature} vecor $[h_1(S), h_2(S), \dots, h_n(S)]$ for $S$. Now, we can form the \textit{signature matrix} $M\prime$, where the $i$-th column of $M$ is replaced by the minhash signature vector. Note that the signature matrix $M\prime$ has the same number of columns as $M$ but only $n$ rows. 
\\
\\
It is not feasible to permute randomly a large characteristic matrix. However, it is possible to simulate a random permutation by a random hash function that map row numbers to as many buckets as there are rows. A hash function that maps integers $0, 1, \dots, n - 1$ to bucket numbers $0, 1, \dots, n - 1$, typically will map some pair of integers to the same bucket and leave some buckets empty.

Thus, instead of picking $n$ random permutations of rows, we pick $n$ randomly chosen hash functions $h_1, h_2, \dots, h_n$. We construct the signature matrix $M\prime$ by considering each row in their given order. Set $\text{SIG}(i, c)$ be the element of the signature matrix for the $i$-th hash function and column $c$. Initially, we set $\text{SIG}(i, c) = \infty$ for all $i$ and $c$. We handle row $r$ by doing the following:

\begin{enumerate}
    \item Compute $h_1(r), h_2(r), \dots, h_n(r)$. For example, $h_1(r) = x + 1 \text{ mod } 5$ and $h_2(r) = 3x + 1 \text{ mod } 5$.
    \item For each column $c$ do the following:
        \begin{itemize}
            \item[(a)] If $c$ has $0$ in row $r$, do nothing.
            \item[(b)] However, if $c$ has $1$ in row $r$, then for each $i = 1, 2, \dots, n$, set $\text{SIG}(i, c) = \min(\text{SIG}(i, c), h_i(r))$.
        \end{itemize}
\end{enumerate}

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c||c|c|}
  \hline
  \textit{Row} & $S_1$ & $S_2$ & $S_3$ & $S_4$ & $x + 1 \text{ mod } 5$ & $3x + 1 \text{ mod } 5$\\
  \hline
  \textit{0} & 1 & 0 & 0 & 1 & 1 & 1 \\
  \textit{1} & 0 & 0 & 1 & 0 & 2 & 4 \\
  \textit{2} & 0 & 1 & 0 & 1 & 3 & 2 \\
  \textit{3} & 1 & 0 & 1 & 1 & 4 & 0 \\
  \textit{4} & 0 & 0 & 1 & 0 & 0 & 3 \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\caption{Hash functions computed for the matrix of Figure \ref{fig:matrix-representation-of-sets}.}
\label{fig:matrix-hash-functions}
\end{figure}

Notice that these two simple hash functions are true permutations of the tows, but a true permutations is only possible because the number of rows, $5$, is prime.
\\
\\
Now, simulate the algorithm for computing the \textbf{signature matrix}. Initially, the matrix has only $\infty$ values. 

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
  $h_2$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-0}
\end{figure}

Consider the first row of the matrix of Figure \ref{fig:matrix-hash-functions}. We see that the values of $h_1(0)$ and $h_2(0)$ are both $1$. In the first row, only $S_1$ and $S_4$ are $1$, so only this columns are updated comparing the values of $h_1(0)$ and $h_2(0)$ with the $\infty$ (current values). The result is the following: 

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $\infty$ & $\infty$ & $1$ \\ 
  $h_2$ & $1$ & $\infty$ & $\infty$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-1}
\end{figure}

Consider the second row of the matrix of Figure \ref{fig:matrix-hash-functions}. This row has value $1$ only in $S_3$, and $h_1(1) = 2$ and $h_2(1) = 4$. Thus, we set $\text{SIG}(1, 3) = 2$ and $\text{SIG}(2, 3) = 4$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $\infty$ & $2$ & $1$ \\ 
  $h_2$ & $1$ & $\infty$ & $4$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-2}
\end{figure}

The row numbered 3 of Figure \ref{fig:matrix-hash-functions} has $1$ in $S_2$ and $S_4$ and the values of $h_1(2) = 3$ and $h_2(2) = 2$. Thus, we set $\text{SIG}(1, 2) = 3$ and $\text{SIG}(2, 2) = 2$. We could also set $\text{SIG}(1, 4) = 3$ and $\text{SIG}(2, 4) = 2$, but we do not do this because we already have a smaller value for $\text{SIG}(1, 4)$ and $\text{SIG}(2, 4)$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $2$ & $1$ \\ 
  $h_2$ & $1$ & $2$ & $4$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

Next comes the row numbered 3 in Figure \ref{fig:matrix-hash-functions}. This row has $1$ in $S_1$, $S_3$ and $S_4$ and the values of $h_1(3) = 4$ and $h_2(3) = 0$. The value of $h_1(3)$ is greater than all values of the first row. However, the value $0$ is smaller then the values that are already present in the second row. Thus, we set $\text{SIG}(2, 1) = 0$ and $\text{SIG}(2, 3) = 0$ and $\text{SIG}(2, 4) = 0$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $2$ & $1$ \\ 
  $h_2$ & $0$ & $3$ & $0$ & $0$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

Finally, consider the row of Figure \ref{fig:matrix-hash-functions} numbered 4. This row has $1$ only in $S_3$ and the values of $h_1(4) = 0$ and $h_2(4) = 3$. Thus, we compare $[2, 0]$ with $[0, 3]$ and set $\text{SIG}(1, 3) = 0$ and keep $\text{SIG}(2, 3) = 0$ because it is already smaller. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $0$ & $1$ \\ 
  $h_2$ & $0$ & $3$ & $0$ & $0$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

We can estimate the Jaccard similarities of the underlying sets from this signature matrix. Column $S_1$ and $S_4$ are identical, so $\text{Jaccard}(S_1, S_4) = 1$ but if we look at Figure \ref{fig:matrix-hash-functions}, we see that the Jaccard similarity of $S_1$ and $S_4$ is $2/3$. This is because the signature matrix is only an approximation of the Jaccard similarities, and this example is to small for the law of the large numbers to ensure that the estimate is close.

\section{Locality-Sensitive Hashing for Documents}\label{sec:lsh-documents}

Even though we can use minhashing to compress large documents into small signatures and preserve the expected similarity of any pair of documents, it still may be impossible to find the pairs with greatest similarity efficiently. The reason is that the number of pairs of documents may be too large, even if there are not too many documents. 

We need to focus our attention only on pairs that are likely to be similar, without investigating all pairs. This is the idea behind \textbf{locality-sensitive hashing} (LSH).

\subsection{LSH for Minhash Signatures}\label{sec:lsh-minhash}

One general approach to LSH it to ``hash'' items several times, doing it in such a way that if two items are similar, then the probability that they will collide is high. We can consider any pair that hashed to the same bucket as a \textit{candidate pair}.

We hope that dissimilar pairs will not collide. Notice that this is not guaranteed and we call \textit{false positives} the pairs that are dissimilar but collide. On the other hand, we call \textit{false negatives} the pairs that are similar but do not collide, and we hope that their number is small.

If we minhash signature for the items, an effective way to choose the hashing is to divide the signature matrix into $b$ bands of $r$ rows each. For each band, we hash the $r$ rows into a hash table with $n$ (big enough) buckets. We can use the same hash function for all bands but we use a different bucket array for each band, so column with the same vector in different buckets will not in the same bucket.


\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/4-signature-matrix-4-bands.tex
}
\caption{Dividing a signature matrix into four bands of three rows per band}
\label{fig:4-signature-matrix-4-bands}
\end{figure}

Figure \ref{fig:4-signature-matrix-4-bands} show part of a signature matrix of $12$ rows divided into four bands of three rows each. The second and fourth columns are identical, so they will be hashed to the same bucket in the first band and the columns that are different will be hashed to different buckets. Two columns that do not agree in Band $1$ have three other chances to become a candidate pair. However, if two columns are similar there is a good chance that they will be hashed to the same bucket in at least one of the bands.

\subsection{Analysis of the Banding Technique}\label{sec:analysis-banding-technique}

Suppose we use $b$ bands of $r$ rows, and suppose that a particular pair of documents have Jaccard similarity $s$. Recall from Section \ref{subsec:minhashing-and-jaccard-similarity} that the probability that two columns agree in a particular row is $s$. We can calculate the probability that two signature become a candidate pair as follows:

\begin{equation*}
    \begin{split}
        \mathbb{P}(\text{agree in all row of one band}) & = s^r \\
        \mathbb{P}(\text{disagree in at least one row of one band}) & = 1 - s^r \\
        \mathbb{P}(\text{disagree in at least one row of all bands}) & = (1 - s^r)^b \\    
        \mathbb{P}(\text{agree in all the rows of a band and become a candidate pair}) & = 1 - (1 - s^r)^b
    \end{split}
\end{equation*}

After choosing $r$ and $b$, this function has the form of an \textit{S-curve} as shown in Figure \ref{fig:4-s-curve}. Where in the $x$-axis we have the Jaccard similarity $s$ and in the $y$-axis we have the probability that two signatures become a candidate pair. 

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/4-s-curve.tex
}
\caption{The S-curve}
\label{fig:4-s-curve}
\end{figure}

The \textit{threshold} of similarity $s$ at which the probability of becoming a candidate pair is $1/2$ is a function of $b$ and $r$. If $b$ and $r$ are large, the threshold is high, so only pairs of documents that are very similar will be candidates. If $b$ and $r$ are small, the threshold is low, so many pairs of documents will be candidates. An approximation of the threshold is $1/b^{1/r}$.

\subsection{Combining the Techniques}\label{sec:combining-techniques}

We now give an approach to finding the set of candidate pairs for similar documents and then discovering the truly similar documents among the candidates. This approach can produce \textit{false positives} and \textit{false negatives}. 

\begin{enumerate}
    \item Pick a value $k$ and construct from each document the set of $k$-shingles. Optionally, hash the $k$-shingles to shorter bucket numbers.
    \item Sort the document-shingle pair to orger them by shingle.
    \item Pick a length $n$ for the minhash signatures. Feed the sorted list to the algorithm of Section \ref{subsec:minhash-signature} to produce the minhash signature for each document.
    \item Choose a threshold $t$ for Jaccard similarity. Pick a number of bands $b$ and a number of rows $r$ per band, such that $br = n$. If the presence of \textit{false negatives} is not acceptable, then choose $b$ and $r$ to produce a threshold lower than $t$.
    \item Construct candidate pairs by applying the LSH technique of Section \ref{sec:lsh-minhash}.
    \item Examine each candidate pair's signatures and determined if the fraction of components in which they agree is at least $t$.
    \item Optionally, if the signature are sufficiently similar, go to the documents themselves and check that they are truly similar.
\end{enumerate}

\section{Distance Measures}\label{sec:distance-measures}

The Jaccard similarity is a measure of how close sets are, it is not really a distance measure. Jaccard distance is defined as $1 - \text{Jaccard similarity}$ and it is a distance measure, but there it is not the only one.

In order to define a distance measure, we need to define a \textit{distance function} $d(x, y)$ that satisfies the following axioms:

\begin{enumerate}
    \item $d(x, y) \geq 0$ for all $x$ and $y$. (no negative distances)
    \item $d(x, y) = 0$ iif $x = y$. (distances are positive, except for identical points)
    \item $d(x, y) = d(y, x)$. (distance is simmetric)
    \item $d(x, z) \leq d(x, y) + d(y, z)$. (triangle inequality)
\end{enumerate}

\subsection{Euclidean Distance}\label{subsec:euclidean-distance}

An \textit{n-dimensional Euclidean space} is one where points are vectors of $n$ real numbers. The Euclidean distance between two points $x$ and $y$ in an $n$-dimensional Euclidean space, which we shall refer to as the $L_2$\textit{-norm}, is defined as:

\begin{equation*}
    \begin{split}
        d(x, y) & = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\\
        \text{where } x & = [x_1, x_2, \dots, x_n] \text{ and }\\ 
                      y & = [y_1, y_2, \dots, y_n]
    \end{split}
\end{equation*}

It is easy to verify the axioms of a distance function for the Euclidean distance. The Euclidean distance cannot be negative because it is the square root of a sum of squares, and the sum of square are strictly positive. On the other hand, the distance will be $0$ if $x_i = y_i$ for all $i$, which means that $x = y$. Simmetry follows because $(x_i - y_i)^2 = -(y_i - x_i)^2$. Finally, the triangle inequality follows from the fact that the square root is a concave function, so the sum of square roots is less than the square root of the sum. 

There are other distance measures that have been used for Euclidean spaces. For any constant $r$, we can define the $L_r$\textit{-norm} as:

\begin{equation*}
    \begin{split}
    d(x, y) & = \left(\sum_{i=1}^{n} \vert x_i - y_i \vert ^ r \right)^{1/r}\\
        \text{where } x & = [x_1, x_2, \dots, x_n] \text{ and }\\ 
                      y & = [y_1, y_2, \dots, y_n]
    \end{split}
\end{equation*}

The $L_1$-norm is also known as the \textit{Manhattan distance} or \textit{city-block distance}. It is the distance between two points in a city where you can only travel along the streets. The $L_\infty$-norm is the limit as $r$ approaches infinity. As $r$ gets larger, only the dimension with the largest difference between $x$ and $y$ matters. Thus, the $L_\infty$-norm is the maximum of $\vert x_i - y_i \vert$ over all $i$.

\subsection{Jaccard Distance}\label{subsec:jaccard-distance}

The \textit{Jaccard distance} of sets $x$ and $y$ is defined as $d(x, y) = 1 - \text{Jaccard}(x, y)$. Recall that the Jaccard similarity of two sets is define as follows:
\begin{equation*}
    \text{Jaccard}(x, y) = \frac{\vert x \cap y \vert}{\vert x \cup y \vert}
\end{equation*}

We have to verify that this is a distance function:
\begin{enumerate}
    \item $d(x, y)$ is non-negative because the size of the intersection cannot exceed the size of the union.
    \item $d(x, y) = 0$ if and only if $x = y$ because $x \cap x = x \cup x = x$.
    \item $d(x, y) = d(y, x)$ because the union and intersection are symmetric.
    \item For the triangle inequality, recall from Section \ref{subsec:minhashing-and-jaccard-similarity} that $\text{Jaccard}(x, y)$ is the probability that a random minhash function maps $x$ and $y$ to the same value. The Jaccard distance $d(x, y)$ is the probability that a random minhash function \textit{does not} send $x$ and $y$ to the same value. We can therefore  translate the condition $d(x, y) \leq d(x, z) + d(z, y)$ to the statement that if $h$ is a random minhash function, then the probability that $h(x) = h(y)$ is at least as large as the probability that $h(x) = h(z)$ plus the probability that $h(z) = h(y)$. This is true because the probability that $h(x) = h(y)$ is at least as large as the probability that $h(x) = h(z)$ plus the probability that $h(z) = h(y)$.
\end{enumerate}

\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

The \textit{cosine similarity} makes sense in spaces that have dimensions, including Euclidean spaces and discrete version of Euclidean spaces, such as spaces where points are vectors with ingeger components of Boolean ($0$ and $1$) components. The cosine distance between two points is the angle that the vectors to those points make with the origin. 

We can calculate the cosine distance by first calculating the cosine of the angle, and then applying the arc-cosine function to translate to an angle in the 0-180 degree range. Given two vectors $x$ and $y$, the cosine of the angle between them is the dot product $x \cdot y$ divided by the $L_2$-norms of $x$ and $y$:
\begin{equation*}
    \begin{split}
        \text{cosine}(x, y) & = \frac{x \cdot y}{\Vert x \Vert \Vert y \Vert}\\
        \text{where } x & = [x_1, x_2, \dots, x_n] \text{ and }\\ 
                      y & = [y_1, y_2, \dots, y_n] \text{ and }\\   
                      x \cdot y & = \sum_{i=1}^{n} x_i y_i\\
    \end{split}
\end{equation*}

\subsection{Edit Distance}\label{subsec:edit-distance}

This distance makes sanse when points are strings. The distance between two strings $x = x_1 x_2 \dots x_n$ and $y = y_1 y_2 \dots y_m$ is the minimum number of insertions and deletions of single characters that will convert $x$ to $y$.

Another way to define and calculate the edit distance $d(x, y)$ is to compute a \textit{longest common subsequence} (LCS) of $x$ and $y$. An LCS of $x$ and $y$ is a string that is constructed by deleting positions from $x$ and $y$, and that is a subsequence of both $x$ and $y$. 

The edit distance $d(x,y)$ can be calculated as the length of $x$ plus the length of $y$ minus twice the length of an LCS of $x$ and $y$.

\subsection{Hamming Distance}\label{subsec:hamming-distance}

Given a space of vectors, we define the \textit{Hamming Distance} between two vecotrs to be the number of components in which they differ. Clearly, the Hamming distance cannot be negative, and it is zero if and only if the two vectors are identical. The triangle inequelity holds because if $x$ and $y$ differ in $k$ components, and $y$ and $z$ differ in $l$ components, then $x$ and $z$ differ in at most $k + l$ components.

Hamming distance is used when the vector are Boolean. 

\section{The Theory of Locality-Sensitive Functions}\label{sec:the-theory-of-locality-sensitive-functions}

The LSH technique developed in Section \ref{sec:lsh-documents} is one example of a family of functions (the minhash functions) that can be combined (by the binding technique) to distinguish strongly between pairs at a low distance from pairs at a high distance.

We shall explore other families of functions that can be used to produce a candidate pair efficiently. These functions can apply to the space of sets and Jaccard distance, or another space and distance function. 

There are three conditions that we need for a family of functions:

\begin{enumerate}
    \item They must be more likely to make close pairs be candidate pairs than distant pairs.
    \item They must be statistically independent, it is possible to estimate the probability that functions will all give the same answer by the product rule for independent events.
    \item They must be easy to compute.
        \begin{itemize}
            \item[(a)] They must be able to identify candidate pairs in time much less than the time take to look at all pairs. For example, minhashing is linear in the number of elements in the set.
            \item[(b)] They must be combinable to build functions that are better at avoiding false negatives and false positives, and the combined functions must be easy to compute.
        \end{itemize}
\end{enumerate}

\subsection{Locality-Sensitive Functions}\label{subsec:locality-sensitive-functions}

We consider functions that take two items and render a decision about whether these items should be a candidate pair. 

In many cases, the function $f$ is a hash function and the decision is based on whether or not the result is equal. We use $f(x) = f(y)$ to denote that $f(x, y) = "yes"$. We call \textit{family} of functions a collection of functions in this form. For example, the family of minhash functions, each based on one of the possible permutations of rows of a characteristic matrix, form a family.

Let $d_1 < d_2$ be two distances. A family $\boldsymbol{\mathcal{F}}$ of functions is said to be $(d_1, d_2, p_1, p_2)$-sensitive if for every $f$ in $\boldsymbol{\mathcal{F}}$: 

\begin{itemize}
    \item If $d(x, y) \leq d_1$, then $\mathbb{P}(f(x) = f(y)) \geq p_1$.
    \item If $d(x, y) \geq d_2$, then $\mathbb{P}(f(x) = f(y)) \leq p_2$.
\end{itemize}

We have only one way to find a family of locality-sensitive functions: use the faimly minhash functions, and assume that the distance is calculated by the Jaccard distance. 

\subsection{Amplifying a Local-Sensitive Family}\label{subsec:amplifying-a-local-sensitive-family}

Suppose we have a given $(d_1, d_2, p_1, p_2)$-sensitive family $\boldsymbol{\mathcal{F}}$. We can construct a new family $\boldsymbol{\mathcal{F'}}$ by the \textit{AND-construction} on $\boldsymbol{\mathcal{F}}$, which is defined as follows. Each member of $\boldsymbol{\mathcal{F'}}$ consists of $r$ members of $\boldsymbol{\mathcal{F}}$ for some $r$. If $f \in \boldsymbol{\mathcal{F}}$ and $f$ is constructed from the set $\{f_1, f_2, \dots, f_r\}$ of members of $\boldsymbol{\mathcal{F}}$, we say $f(x) = f(y)$ if and only if $f_i(x) = f_i(y)$ for all $i = 1, 2, \dots, r$.

Since the members of $\boldsymbol{\mathcal{F}}$ are independently chosen to make a mamber of  $\boldsymbol{\mathcal{F'}}$, we can assert that $\boldsymbol{\mathcal{F'}}$ is a $(d_1, d_2, (p_1)^r, (p_2)^r)$-sensitive family.

There is another construction, which we call the \textit{OR-construction}, that turns a $(d_1, d_2, p_1, p_2)$-sensitive family $\boldsymbol{\mathcal{F}}$ into a $(d_1, d_2, 1 - (1 - p_1)^b, 1 - (1 - p_2)^b)$-sensitive family $\boldsymbol{\mathcal{F'}}$. We define $f(x) = f(y)$ if and only if $f_i(x) = f_i(y)$ for at least one $i = 1, 2, \dots, b$.

If $p$ is the probability that a member of $\boldsymbol{\mathcal{F}}$ will declare $(x, y)$ to be a candidate pair, then $1 - p$ is the probability that it will not. $1 - p^b$ is the probability that none of $\{f_1, f_2, \dots, f_b\}$ will declare $(x, y)$ to be a candidate pair. Therefore, $1 - (1 - p)^b$ is the probability that at least one of $\{f_1, f_2, \dots, f_b\}$ will declare $(x, y)$ to be a candidate pair.

We can combine the AND-construction and the OR-construction in any order to make the lower probability close to $0$ and the higher probability close to $1$.
\\
\\
For example, suppose we start a family $\boldsymbol{\mathcal{F}}$. We use the AND-construction with $r = 4$ to produce a family $\boldsymbol{\mathcal{F'}}$. We then apply the OR-construction to $\boldsymbol{\mathcal{F'}}$ with $b = 4$ to produce a third family $\boldsymbol{\mathcal{F''}}$. 

It means that $\boldsymbol{\mathcal{F'}}$ is a quadruple of functions from $\boldsymbol{\mathcal{F}}$, and $\boldsymbol{\mathcal{F''}}$ is a quadruple of quadruples of functions from $\boldsymbol{\mathcal{F}}$. Thus, $\boldsymbol{\mathcal{F''}}$ is a family of $16$ functions from $\boldsymbol{\mathcal{F}}$.

\begin{equation*}
    \begin{split}
        \boldsymbol{\mathcal{F}}   & = (0.2, 0.6, 0.8, 0.1)-sensitive \\
        \boldsymbol{\mathcal{F''}} & = (0.2, 0.6, 1 - (1 - 0.8)^4, 1 - (1 - 0.1)^4)-sensitive \\
                                   & = (0.2, 0.6, 0.8785, 0.0985)-sensitive 
    \end{split}
\end{equation*}

\subsection{LSH Families for Hamming Distance}\label{subsec:lsh-families-for-hamming-distance}

Suppose we have a space of $d$-dimensional vectors, and $h(x, y)$ denotes the Hamming distance between $x$ and $y$. We can define $f_i(x)$ to be the $i$-th bit of $x$. Then, $f_i(x) = f_i(y)$ if and only if $x$ and $y$ agree in the $i$-th bit. The probability that $f_i(x) = f_i(y)$ is $1 - h(x, y) / d$. This situation is almost like the one we encountered for minhashing.

Thus, the family $\boldsymbol{\mathcal{F}}$ of functions $\{f_1, f_2, \dots, f_d\}$ is $(d_1, d_2, 1- p_1/d, 1 - p_2/d)$-sensitive faimly of hash functions, for any $d_1 < d_2$.

This family is composed by $d$ functions and this is limiting to apply AND-construction or OR-construction. 

\subsection{Random Hyperplanes and the Cosine Distance}\label{subsec:random-hyperplanes-and-the-cosine-distance}

Recall from Section \ref{subsec:cosine-similarity} that the cosine distance between two vectors is the angle between the two vectors. 

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/4-two-vectors.tex
}
\caption{Two vectors make an angle $\theta$}
\label{fig:4-two-vectors}
\end{figure}

Suppose we pick a hyperplane through the origin. The hyperplane divides the space into two half-spaces. First condider a vector $v$ that is the dashed line in Figure \ref{fig:4-two-vectors}, note that $x$ and $y$ are on different sides of the hyperplane. Then the dot product $v \cdot x$ and $v \cdot y$ have opposite signs.

On the other hand, the randomly chosen vector $v$ could be normal to a hyperplane like the dotted line in Figure \ref{fig:4-two-vectors}. In this case, $x$ and $y$ are on the same side of the hyperplane. Then the dot product $v \cdot x$ and $v \cdot y$ have the same sign.

The probability that $v \cdot x$ and $v \cdot y$ have the same sign is $\theta / 180$. Thus, each hash function $f$ in our locality-sensitive family $\boldsymbol{\mathcal{F}}$ is defined by a randomly chosen vector $v_f$. Given two vectors $x$ and $y$, then $f(x) = f(y)$ if and only if $v_f \cdot x$ and $v_f \cdot y$ have the same sign. The parameters are essentially the same as for the Jaccard-distance family. That is, $\boldsymbol{\mathcal{F}}$ is a $(d_1, d_2, (180-d_1)/180, (180-d_2)/180)$-sensitive family.

\subsection{LSH Families for Euclidean Distance}\label{subsec:lsh-families-for-euclidean-distance}

We will start with a 2-dimensional Euclidean space. Each hash function $f$ in our family $\boldsymbol{\mathcal{F}}$ will be associated with a randomly chosen line in this space. Pick a constant $a$ and divide the line into segments of length $a$, as shown in Figure \ref{fig:4-two-points-and-buckets}.       

The segment of the line are the buckets into which function $f$ hashed the points. A point is hashed to the bucket in which its projection onto the line lies. If the distance $d$ between two points is small compared to $a$, then there is a good chance that the two points will be hashed to the same bucket. If $d$ is large compared to $a$, then there is a small chance that the two points will be hashed to the same bucket.

However, suppose $d$ is larger the $a$. In order for there to be any chance of two points falling in the same bucket, we need $d\cos{\theta} < a$, where $\theta$ is the angle between the line and the line connecting the two points.

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/4-two-points-and-buckets.tex
}
\caption{Two points at distance $d \gg a$ have a small chance of being hashed to the same bucket}
\label{fig:4-two-points-and-buckets}
\end{figure}

If $d \geq 2a$, then there is no more than a $1/3$ chance the two points fall in the same bucket. The reason is that for $\cos{\theta}$ to be less than $1/2$, we need to have $\theta > 60^{\circ}$ and $\theta < 90^{\circ}$. If $\theta > 0^{\circ}$ and $\theta < 60^{\circ}$, then $\cos{\theta} > 1/2$. But since $\theta$ is the smaller angle between two randomly chosen lines in the plane, $\theta$ is twice as likely to be between $0^{\circ}$ and $60^{\circ}$ as it is to be between $60^{\circ}$ and $90^{\circ}$.

We conclude that the family $\boldsymbol{\mathcal{F}}$ just described froms a $(a/2, 2a, 1/2, 1/3)$-sensitive family of hash functions. That is, for istances up to $a/2$ the probability that two points are hashed to the same bucket is at least $1/2$, while for distances greater than $2a$ the probability is at most $1/3$.
