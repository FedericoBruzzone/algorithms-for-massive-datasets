\chapter{Finding Similar Items}\label{sec:finding-similar-items}

We show how the problem of finding textually similar documents can be turned into such a set problem by the technique known as ``shingling''. We introduce a technique called ``minhashing'', which compresses large sets in such a way that we can still deduce the similarity of the underlying sets from their compressed versions. Another important problem that arises when we search for similar items of any kind is the problem of ``nearest neighbor search''. We show how this problem can be solved by a technique called ``Locality Sensitive Hashing''. 

\section{Application of Near-Neighbor Search}\label{sec:application-of-near-neighbor-search}

We shall focus initially on a particular notion of ``similarity'': the similarity of sets by looking at the relative size of their intersection. This notion is called \textit{Jaccard similarity}:
\begin{equation*}
    \text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation*}
where $A$ and $B$ are sets. The Jaccard similarity is a number between 0 and 1, where 0 means that the two sets are disjoint, and 1 means that they are equal.

\subsection{Similarity of Documents}\label{subsec:similarity-of-documents}

We can use the Jaccard similarity to measure the similarity of documents. We can represent a document as a set of words that occur in it. We can then measure the similarity of two documents by measuring the similarity of the sets of words that occur in them.

In the base case, where the two documents are exact duplicates, we can compare them character by character. However in many applications, the document are not identical, but they share a lot of part of text.

\begin{itemize}
    \item Plagiarism: finding plagiarized documents tests our ability to find textually similar documents.
    \item Mirror Pages: a mirror page is a page that is identical to another page, except for some minor changes, such as the replacement of the author's name or the addition of a few words.
    \item Articles from the Same Source: we may want to find articles from the same source, such as the New York Times.
\end{itemize}

\subsection{Collaborative Filtering as a Similar-Sets Problem}\label{subsec:collaborative-filtering-as-a-similar-sets-problem}

Another class of applications where similarity of sets is very important is called \textit{collaborative filtering}, a process where we raccomend to users items that other similar users have liked. 
\\
\\
\noindent \textbf{On-line Purchases} 

\noindent We can use collaborative filtering to recommend products to users based on the products that other similar users have purchased. We can say that two users are similar if their sets of purchased products have a high Jaccard similarity. 

Collaborative filtering requires us to find the nearest neighbors of a given user. However, by combining the similarity-finding with clustering, we are able to find the nearest neighbors of a user very quickly.
\\
\\
\noindent \textbf{Movie Ratings}

\noindent We can see movies as similar if they were rented or rated highly by many of the same customers, and see customers as similar if they rented or rated highly many of the same movies. 

When out data consists of rating rather then binary decisions, we cannot use the sets of items. Some alternative approaches are:

\begin{enumerate}
    \item Ignore low-rated customer/movie pairs.
    \item Use a threshold to convert ratings to binary decisions.
    \item If ratins are on a scale of 1 to 5, we can put in a multi-set the number of times each movie was rated. In order to compute Jaccard similarity of a multi-set, we can use the following formula:
    \begin{equation*}
        \text{Jaccard}(A, B) = \frac{\sum_{i=1}^n \min(A_i, B_i)}{\sum_{i=1}^n \max(A_i, B_i)}
    \end{equation*}
\end{enumerate}


\section{Shingling of Documents}\label{sec:shingling-of-documents}

The most effective way to represent documents as sets is to construct from the document the set of short string of characters that appear within it. These strings are called \textit{shingles}. 

A \textit{k}-shingle of a document is simply a sequence of $k$ characters that appears in it. For example, suppose that $D$ is the document containing the string ``\texttt{abcdadb}'' and that $k = 2$. Then the set of 2-shingles for $D$ is $\{ \texttt{ab}, \texttt{bc}, \texttt{cd}, \texttt{da}, \texttt{db} \}$. 

\subsection{Choosing the Shingle Size}\label{subsec:choosing-the-shingle-size}

We can pick $k$ to be any constant we like. But, if $k$ is too small, then the Jaccard similarity between two documents will be very high, even if the documents are completely different. On the other hand, if $k$ is too large, then we will miss the similarity between documents that share a lot of common text.

The important thing to remember is that, $k$ should be picked large enough that the probability of a given shingle appearning in any given document is low. 

A good rule of thumb is to immagine that there are only 20 characters and estimate the number of $k$-shingles as $20^k$. For example, for an email we can choose $k = 5$, since $20^5 = 3.2$ million and an email is unlikely to contain more than a few million characters. For a web page, we can choose $k = 9$, since $20^9 = 5.4$ trillion and a web page is unlikely to contain more than a few trillion characters.

\subsection{Hashing Shingles}\label{subsec:hashing-shingles}

Instead of using substring directly as shingles, we can pick a hash function $h$ and use $h(S)$ as the shingle for any string $S$. For instance, we can pick $k = 9$ and use the hash function $h(S) = \{0, 1, \dots, 2^{32} - 1\}$, we need to store only 4 bytes for each shingle. 

Notice that we can differentiate documents better if we use 9-shingles and hash them down to four bytes than to use 4-shingles, even though the space used to represent a shingle is the same.

\subsection{Shingles Built from Words}\label{subsec:shingles-built-from-words}

An alternative form of shingle has proved effective for the problem of identifying similar news articles, as mentioned in Section \ref{subsec:similarity-of-documents}. The exploitable distinction for this problem is that the news articles are written in a rather different style than are other elements that typically appear on the page with the article. However, for the problem of finding similar news articles, it was found that defining a shingle to be a stop word followed by the next two words, regardless of whether or not they were stop words, formed a useful set of shingles. 

\section{Similarity-Preserving Summaries of Sets}\label{sec:similarity-preserving-summaries-of-sets}

Set of shingles are large. Even if we hash them to four bytes each, the set needed to store a set is still large. The goal of this section is to replace large sets by much smaller representation called ``signatures'' that preserve the similarity of the sets.

The important property we need for signature is that we can compare the signature of two sets and get a good estimate of their Jaccard similarity. It is not possible that the signature of two sets are identical but their Jaccard similarity is low.

\subsection{Matrix Representation of Sets}\label{subsec:matrix-representation-of-sets}

Before explaining how it is possible create a small signature from a large set, we need to introduce a matrix representation of sets. The columns of the matrix correspond to the sets, and the row correspond to the elements of the universal sets from which elements of the sets are drawn.
Given a row $i$ and a column $j$, the entry $M_{ij}$ is 1 if the element $i$ is in the set $j$ and 0 otherwise.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
  \hline
  \textit{Element} & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  \textit{a} & 1 & 0 & 0 & 1  \\
  \textit{b} & 0 & 0 & 1 & 0  \\
  \textit{c} & 0 & 1 & 0 & 1  \\
  \textit{d} & 1 & 0 & 1 & 1  \\
  \textit{e} & 0 & 0 & 1 & 0  \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\caption{Matrix representation of sets}
\label{fig:matrix-representation-of-sets}
\end{figure}

It is important remember that the characteristic matrix is unlikely to be the way the data is stored in practice. The matrix representation is just a convenient way to think about sets.

\subsection{Minhashing}\label{subsec:minhashing}

The signature we deisre to construct for sets are composed of the results of a large number of calulation each of which is a ``minhash`` of the characteristic matrix. 

To \textit{minhash} a set represented by a column of the characteristic matrix, we first randomly permute the rows of the matrix. Then, the minhash value of the set is the number of the first row in which the set has a 1. 

\begin{equation*}
    \begin{split}
        h(S) &= \min_{i \in S} \boldsymbol{\pi}(i)\\
        \text{where } \boldsymbol{\pi}(x) &= \alpha x + \beta \text{ mod } n\\ 
        \text{and } \alpha, \beta &\text{ are random integers,}\\
        n & \text{ is number of rows in the matrix}
    \end{split}
\end{equation*}

\subsection{Minhashing and Jaccard Similarity}\label{subsec:minhashing-and-jaccard-similarity}

The minhashing technique has the property that the probability that the minhash of two sets is the same is equal to the Jaccard similarity of the sets.

To see why, we need to picture the columns for those two sets. If we restrict our attention to the column $S_1$ and $S_2$, then rows can divided into three categories:

\begin{enumerate}
    \item $X$ rows have 1 in both $S_1$ and $S_2$. 
    \item $Y$ rows have 1 in only one of $S_1$ and $S_2$ and 0 in the other.
    \item $Z$ rows have 0 in both $S_1$ and $S_2$.
\end{enumerate}

Remembering that the matrix is sparse, most rows are in category 3 ($Z$). However, it is the ratio between $X$ and $Y$ that determines the $\text{Jaccard}(S_1, S_2)$ and the $\mathbb{P}(h(S_1) = h(S_2))$.

Let $x \in X$ and $y \in Y$, then:

\begin{equation*}
    \begin{split}
        \text{Jaccard}(S_1, S_2) &= x / (x + y)\\
        \text{where } x &= |S_1 \cap S_2|\\
        x + y & = |S_1 \cup S_2|
    \end{split}
\end{equation*}

Consider the probability, immagine the rows permuted randomly and proceed from top. The $\mathbb{P}(\text{we meet a row in } X \text{ before we meet a row in } Y) = x / (x + y)$. If the first row we meet is in $X$, then $h(S_1) = h(S_2)$, but if the first row we meet is in $Y$, then $h(S_1) \neq h(S_2)$. We can conclude that:

\begin{equation*}
    \begin{split}
        \mathbb{P}(h(S_1) = h(S_2)) &= \mathbb{P}(\text{we meet a row in } X \text{ before we meet a row in } Y)\\
        &= x / (x + y)\\
        &= \text{Jaccard}(S_1, S_2)
    \end{split}
\end{equation*}

\subsection{Minhash Signature}\label{subsec:minhash-signature}

Let $M$ our characteristic matrix. To represent sets, we pick at random some number $n$ of premutations of the rows of $M$. Call the minhash functions determined by these permutations $h_1, h_2, \dots, h_n$. From the column representing set $S$, construct the \textit{minhash signature} vecor $[h_1(S), h_2(S), \dots, h_n(S)]$ for $S$. Now, we can form the \textit{signature matrix} $M\prime$, where the $i$-th column of $M$ is replaced by the minhash signature vector. Note that the signature matrix $M\prime$ has the same number of columns as $M$ but only $n$ rows. 
\\
\\
It is not feasible to permute randomly a large characteristic matrix. However, it is possible to simulate a random permutation by a random hash function that map row numbers to as many buckets as there are rows. A hash function that maps integers $0, 1, \dots, n - 1$ to bucket numbers $0, 1, \dots, n - 1$, typically will map some pair of integers to the same bucket and leave some buckets empty.

Thus, instead of picking $n$ random permutations of rows, we pick $n$ randomly chosen hash functions $h_1, h_2, \dots, h_n$. We construct the signature matrix $M\prime$ by considering each row in their given order. Set $\text{SIG}(i, c)$ be the element of the signature matrix for the $i$-th hash function and column $c$. Initially, we set $\text{SIG}(i, c) = \infty$ for all $i$ and $c$. We handle row $r$ by doing the following:

\begin{enumerate}
    \item Compute $h_1(r), h_2(r), \dots, h_n(r)$. For example, $h_1(r) = x + 1 \text{ mod } 5$ and $h_2(r) = 3x + 1 \text{ mod } 5$.
    \item For each column $c$ do the following:
        \begin{itemize}
            \item[(a)] If $c$ has $0$ in row $r$, do nothing.
            \item[(b)] However, if $c$ has $1$ in row $r$, then for each $i = 1, 2, \dots, n$, set $\text{SIG}(i, c) = \min(\text{SIG}(i, c), h_i(r))$.
        \end{itemize}
\end{enumerate}

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c||c|c|}
  \hline
  \textit{Row} & $S_1$ & $S_2$ & $S_3$ & $S_4$ & $x + 1 \text{ mod } 5$ & $3x + 1 \text{ mod } 5$\\
  \hline
  \textit{0} & 1 & 0 & 0 & 1 & 1 & 1 \\
  \textit{1} & 0 & 0 & 1 & 0 & 2 & 4 \\
  \textit{2} & 0 & 1 & 0 & 1 & 3 & 2 \\
  \textit{3} & 1 & 0 & 1 & 1 & 4 & 0 \\
  \textit{4} & 0 & 0 & 1 & 0 & 0 & 3 \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\caption{Hash functions computed for the matrix of Figure \ref{fig:matrix-representation-of-sets}.}
\label{fig:matrix-hash-functions}
\end{figure}

Notice that these two simple hash functions are true permutations of the tows, but a true permutations is only possible because the number of rows, $5$, is prime.
\\
\\
Now, simulate the algorithm for computing the \textbf{signature matrix}. Initially, the matrix has only $\infty$ values. 

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
  $h_2$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-0}
\end{figure}

Consider the first row of the matrix of Figure \ref{fig:matrix-hash-functions}. We see that the values of $h_1(0)$ and $h_2(0)$ are both $1$. In the first row, only $S_1$ and $S_4$ are $1$, so only this columns are updated comparing the values of $h_1(0)$ and $h_2(0)$ with the $\infty$ (current values). The result is the following: 

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $\infty$ & $\infty$ & $1$ \\ 
  $h_2$ & $1$ & $\infty$ & $\infty$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-1}
\end{figure}

Consider the second row of the matrix of Figure \ref{fig:matrix-hash-functions}. This row has value $1$ only in $S_3$, and $h_1(1) = 2$ and $h_2(1) = 4$. Thus, we set $\text{SIG}(1, 3) = 2$ and $\text{SIG}(2, 3) = 4$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $\infty$ & $2$ & $1$ \\ 
  $h_2$ & $1$ & $\infty$ & $4$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-2}
\end{figure}

The row numbered 3 of Figure \ref{fig:matrix-hash-functions} has $1$ in $S_2$ and $S_4$ and the values of $h_1(2) = 3$ and $h_2(2) = 2$. Thus, we set $\text{SIG}(1, 2) = 3$ and $\text{SIG}(2, 2) = 2$. We could also set $\text{SIG}(1, 4) = 3$ and $\text{SIG}(2, 4) = 2$, but we do not do this because we already have a smaller value for $\text{SIG}(1, 4)$ and $\text{SIG}(2, 4)$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $2$ & $1$ \\ 
  $h_2$ & $1$ & $2$ & $4$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

Next comes the row numbered 3 in Figure \ref{fig:matrix-hash-functions}. This row has $1$ in $S_1$, $S_3$ and $S_4$ and the values of $h_1(3) = 4$ and $h_2(3) = 0$. The value of $h_1(3)$ is greater than all values of the first row. However, the value $0$ is smaller then the values that are already present in the second row. Thus, we set $\text{SIG}(2, 1) = 0$ and $\text{SIG}(2, 3) = 0$ and $\text{SIG}(2, 4) = 0$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $2$ & $1$ \\ 
  $h_2$ & $0$ & $3$ & $0$ & $0$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

Finally, consider the row of Figure \ref{fig:matrix-hash-functions} numbered 4. This row has $1$ only in $S_3$ and the values of $h_1(4) = 0$ and $h_2(4) = 3$. Thus, we compare $[2, 0]$ with $[0, 3]$ and set $\text{SIG}(1, 3) = 0$ and keep $\text{SIG}(2, 3) = 0$ because it is already smaller. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $0$ & $1$ \\ 
  $h_2$ & $0$ & $3$ & $0$ & $0$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

We can estimate the Jaccard similarities of the underlying sets from this signature matrix. Column $S_1$ and $S_4$ are identical, so $\text{Jaccard}(S_1, S_4) = 1$ but if we look at Figure \ref{fig:matrix-hash-functions}, we see that the Jaccard similarity of $S_1$ and $S_4$ is $2/3$. This is because the signature matrix is only an approximation of the Jaccard similarities, and this example is to small for the law of the large numbers to ensure that the estimate is close.

\section{Locality-Sensitive Hashing for Documents}\label{sec:lsh-documents}

Even though we can use minhashing to compress large documents into small signatures and preserve the expected similarity of any pair of documents, it still may be impossible to find the pairs with greatest similarity efficiently. The reason is that the number of pairs of documents may be too large, even if there are not too many documents. 

We need to focus our attention only on pairs that are likely to be similar, without investigating all pairs. This is the idea behind \textbf{locality-sensitive hashing} (LSH).

\subsection{LSH for Minhash Signatures}\label{sec:lsh-minhash}
