\chapter{Finding Similar Items}\label{sec:finding-similar-items}

We show how the problem of finding textually similar documents can be turned into such a set problem by the technique known as ``shingling''. We introduce a technique called ``minhashing'', which compresses large sets in such a way that we can still deduce the similarity of the underlying sets from their compressed versions. Another important problem that arises when we search for similar items of any kind is the problem of ``nearest neighbor search''. We show how this problem can be solved by a technique called ``Locality Sensitive Hashing''. 

\section{Application of Near-Neighbor Search}\label{sec:application-of-near-neighbor-search}

We shall focus initially on a particular notion of ``similarity'': the similarity of sets by looking at the relative size of their intersection. This notion is called \textit{Jaccard similarity}:
\begin{equation*}
    \text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation*}
where $A$ and $B$ are sets. The Jaccard similarity is a number between 0 and 1, where 0 means that the two sets are disjoint, and 1 means that they are equal.

\subsection{Similarity of Documents}\label{subsec:similarity-of-documents}

We can use the Jaccard similarity to measure the similarity of documents. We can represent a document as a set of words that occur in it. We can then measure the similarity of two documents by measuring the similarity of the sets of words that occur in them.

In the base case, where the two documents are exact duplicates, we can compare them character by character. However in many applications, the document are not identical, but they share a lot of part of text.

\begin{itemize}
    \item Plagiarism: finding plagiarized documents tests our ability to find textually similar documents.
    \item Mirror Pages: a mirror page is a page that is identical to another page, except for some minor changes, such as the replacement of the author's name or the addition of a few words.
    \item Articles from the Same Source: we may want to find articles from the same source, such as the New York Times.
\end{itemize}

\subsection{Collaborative Filtering as a Similar-Sets Problem}\label{subsec:collaborative-filtering-as-a-similar-sets-problem}

Another class of applications where similarity of sets is very important is called \textit{collaborative filtering}, a process where we raccomend to users items that other similar users have liked. 
\\
\\
\noindent \textbf{On-line Purchases} 

\noindent We can use collaborative filtering to recommend products to users based on the products that other similar users have purchased. We can say that two users are similar if their sets of purchased products have a high Jaccard similarity. 

Collaborative filtering requires us to find the nearest neighbors of a given user. However, by combining the similarity-finding with clustering, we are able to find the nearest neighbors of a user very quickly.
\\
\\
\noindent \textbf{Movie Ratings}

\noindent We can see movies as similar if they were rented or rated highly by many of the same customers, and see customers as similar if they rented or rated highly many of the same movies. 

When out data consists of rating rather then binary decisions, we cannot use the sets of items. Some alternative approaches are:

\begin{enumerate}
    \item Ignore low-rated customer/movie pairs.
    \item Use a threshold to convert ratings to binary decisions.
    \item If ratins are on a scale of 1 to 5, we can put in a multi-set the number of times each movie was rated. In order to compute Jaccard similarity of a multi-set, we can use the following formula:
    \begin{equation*}
        \text{Jaccard}(A, B) = \frac{\sum_{i=1}^n \min(A_i, B_i)}{\sum_{i=1}^n \max(A_i, B_i)}
    \end{equation*}
\end{enumerate}


\section{Shingling of Documents}\label{sec:shingling-of-documents}

The most effective way to represent documents as sets is to construct from the document the set of short string of characters that appear within it. These strings are called \textit{shingles}. 

A \textit{k}-shingle of a document is simply a sequence of $k$ characters that appears in it. For example, suppose that $D$ is the document containing the string ``\texttt{abcdadb}'' and that $k = 2$. Then the set of 2-shingles for $D$ is $\{ \texttt{ab}, \texttt{bc}, \texttt{cd}, \texttt{da}, \texttt{db} \}$. 

\subsection{Choosing the Shingle Size}\label{subsec:choosing-the-shingle-size}

We can pick $k$ to be any constant we like. But, if $k$ is too small, then the Jaccard similarity between two documents will be very high, even if the documents are completely different. On the other hand, if $k$ is too large, then we will miss the similarity between documents that share a lot of common text.

The important thing to remember is that, $k$ should be picked large enough that the probability of a given shingle appearning in any given document is low. 

A good rule of thumb is to immagine that there are only 20 characters and estimate the number of $k$-shingles as $20^k$. For example, for an email we can choose $k = 5$, since $20^5 = 3.2$ million and an email is unlikely to contain more than a few million characters. For a web page, we can choose $k = 9$, since $20^9 = 5.4$ trillion and a web page is unlikely to contain more than a few trillion characters.

\subsection{Hashing Shingles}\label{subsec:hashing-shingles}

Instead of using substring directly as shingles, we can pick a hash function $h$ and use $h(S)$ as the shingle for any string $S$. For instance, we can pick $k = 9$ and use the hash function $h(S) = \{0, 1, \dots, 2^{32} - 1\}$, we need to store only 4 bytes for each shingle. 

Notice that we can differentiate documents better if we use 9-shingles and hash them down to four bytes than to use 4-shingles, even though the space used to represent a shingle is the same.

\subsection{Shingles Built from Words}\label{subsec:shingles-built-from-words}

An alternative form of shingle has proved effective for the problem of identifying similar news articles, as mentioned in Section \ref{subsec:similarity-of-documents}. The exploitable distinction for this problem is that the news articles are written in a rather different style than are other elements that typically appear on the page with the article. However, for the problem of finding similar news articles, it was found that defining a shingle to be a stop word followed by the next two words, regardless of whether or not they were stop words, formed a useful set of shingles. 

\section{Similarity-Preserving Summaries of Sets}\label{sec:similarity-preserving-summaries-of-sets}

Set of shingles are large. Even if we hash them to four bytes each, the set needed to store a set is still large. The goal of this section is to replace large sets by much smaller representation called ``signatures'' that preserve the similarity of the sets.

The important property we need for signature is that we can compare the signature of two sets and get a good estimate of their Jaccard similarity. It is not possible that the signature of two sets are identical but their Jaccard similarity is low.

\subsection{Matrix Representation of Sets}\label{subsec:matrix-representation-of-sets}

Before explaining how it is possible create a small signature from a large set, we need to introduce a matrix representation of sets. The columns of the matrix correspond to the sets, and the row correspond to the elements of the universal sets from which elements of the sets are drawn.
Given a row $i$ and a column $j$, the entry $M_{ij}$ is 1 if the element $i$ is in the set $j$ and 0 otherwise.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
  \hline
  \textit{Element} & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  \textit{a} & 1 & 0 & 0 & 1  \\
  \textit{b} & 0 & 0 & 1 & 0  \\
  \textit{c} & 0 & 1 & 0 & 1  \\
  \textit{d} & 1 & 0 & 1 & 1  \\
  \textit{e} & 0 & 0 & 1 & 0  \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\caption{Matrix representation of sets}
\label{fig:matrix-representation-of-sets}
\end{figure}

It is important remember that the characteristic matrix is unlikely to be the way the data is stored in practice. The matrix representation is just a convenient way to think about sets.

\subsection{Minhashing}\label{subsec:minhashing}

The signature we deisre to construct for sets are composed of the results of a large number of calulation each of which is a ``minhash`` of the characteristic matrix. 

To \textit{minhash} a set represented by a column of the characteristic matrix, we first randomly permute the rows of the matrix. Then, the minhash value of the set is the number of the first row in which the set has a 1. 

\begin{equation*}
    \begin{split}
        h(S) &= \min_{i \in S} \boldsymbol{\pi}(i)\\
        \text{where } \boldsymbol{\pi}(x) &= \alpha x + \beta \text{ mod } n\\ 
        \text{and } \alpha, \beta &\text{ are random integers,}\\
        n & \text{ is number of rows in the matrix}
    \end{split}
\end{equation*}

\subsection{Minhashing and Jaccard Similarity}\label{subsec:minhashing-and-jaccard-similarity}

The minhashing technique has the property that the probability that the minhash of two sets is the same is equal to the Jaccard similarity of the sets.

To see why, we need to picture the columns for those two sets. If we restrict our attention to the column $S_1$ and $S_2$, then rows can divided into three categories:

\begin{enumerate}
    \item $X$ rows have 1 in both $S_1$ and $S_2$. 
    \item $Y$ rows have 1 in only one of $S_1$ and $S_2$ and 0 in the other.
    \item $Z$ rows have 0 in both $S_1$ and $S_2$.
\end{enumerate}

Remembering that the matrix is sparse, most rows are in category 3 ($Z$). However, it is the ratio between $X$ and $Y$ that determines the $\text{Jaccard}(S_1, S_2)$ and the $\mathbb{P}(h(S_1) = h(S_2))$.

Let $x \in X$ and $y \in Y$, then:

\begin{equation*}
    \begin{split}
        \text{Jaccard}(S_1, S_2) &= x / (x + y)\\
        \text{where } x &= |S_1 \cap S_2|\\
        x + y & = |S_1 \cup S_2|
    \end{split}
\end{equation*}

Consider the probability, immagine the rows permuted randomly and proceed from top. The $\mathbb{P}(\text{we meet a row in } X \text{ before we meet a row in } Y) = x / (x + y)$. If the first row we meet is in $X$, then $h(S_1) = h(S_2)$, but if the first row we meet is in $Y$, then $h(S_1) \neq h(S_2)$. We can conclude that:

\begin{equation*}
    \begin{split}
        \mathbb{P}(h(S_1) = h(S_2)) &= \mathbb{P}(\text{we meet a row in } X \text{ before we meet a row in } Y)\\
        &= x / (x + y)\\
        &= \text{Jaccard}(S_1, S_2)
    \end{split}
\end{equation*}

\subsection{Minhash Signature}\label{subsec:minhash-signature}

Let $M$ our characteristic matrix. To represent sets, we pick at random some number $n$ of premutations of the rows of $M$. Call the minhash functions determined by these permutations $h_1, h_2, \dots, h_n$. From the column representing set $S$, construct the \textit{minhash signature} vecor $[h_1(S), h_2(S), \dots, h_n(S)]$ for $S$. Now, we can form the \textit{signature matrix} $M\prime$, where the $i$-th column of $M$ is replaced by the minhash signature vector. Note that the signature matrix $M\prime$ has the same number of columns as $M$ but only $n$ rows. 
\\
\\
It is not feasible to permute randomly a large characteristic matrix. However, it is possible to simulate a random permutation by a random hash function that map row numbers to as many buckets as there are rows. A hash function that maps integers $0, 1, \dots, n - 1$ to bucket numbers $0, 1, \dots, n - 1$, typically will map some pair of integers to the same bucket and leave some buckets empty.

Thus, instead of picking $n$ random permutations of rows, we pick $n$ randomly chosen hash functions $h_1, h_2, \dots, h_n$. We construct the signature matrix $M\prime$ by considering each row in their given order. Set $\text{SIG}(i, c)$ be the element of the signature matrix for the $i$-th hash function and column $c$. Initially, we set $\text{SIG}(i, c) = \infty$ for all $i$ and $c$. We handle row $r$ by doing the following:

\begin{enumerate}
    \item Compute $h_1(r), h_2(r), \dots, h_n(r)$. For example, $h_1(r) = x + 1 \text{ mod } 5$ and $h_2(r) = 3x + 1 \text{ mod } 5$.
    \item For each column $c$ do the following:
        \begin{itemize}
            \item[(a)] If $c$ has $0$ in row $r$, do nothing.
            \item[(b)] However, if $c$ has $1$ in row $r$, then for each $i = 1, 2, \dots, n$, set $\text{SIG}(i, c) = \min(\text{SIG}(i, c), h_i(r))$.
        \end{itemize}
\end{enumerate}

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c||c|c|}
  \hline
  \textit{Row} & $S_1$ & $S_2$ & $S_3$ & $S_4$ & $x + 1 \text{ mod } 5$ & $3x + 1 \text{ mod } 5$\\
  \hline
  \textit{0} & 1 & 0 & 0 & 1 & 1 & 1 \\
  \textit{1} & 0 & 0 & 1 & 0 & 2 & 4 \\
  \textit{2} & 0 & 1 & 0 & 1 & 3 & 2 \\
  \textit{3} & 1 & 0 & 1 & 1 & 4 & 0 \\
  \textit{4} & 0 & 0 & 1 & 0 & 0 & 3 \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\caption{Hash functions computed for the matrix of Figure \ref{fig:matrix-representation-of-sets}.}
\label{fig:matrix-hash-functions}
\end{figure}

Notice that these two simple hash functions are true permutations of the tows, but a true permutations is only possible because the number of rows, $5$, is prime.
\\
\\
Now, simulate the algorithm for computing the \textbf{signature matrix}. Initially, the matrix has only $\infty$ values. 

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
  $h_2$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-0}
\end{figure}

Consider the first row of the matrix of Figure \ref{fig:matrix-hash-functions}. We see that the values of $h_1(0)$ and $h_2(0)$ are both $1$. In the first row, only $S_1$ and $S_4$ are $1$, so only this columns are updated comparing the values of $h_1(0)$ and $h_2(0)$ with the $\infty$ (current values). The result is the following: 

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $\infty$ & $\infty$ & $1$ \\ 
  $h_2$ & $1$ & $\infty$ & $\infty$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-1}
\end{figure}

Consider the second row of the matrix of Figure \ref{fig:matrix-hash-functions}. This row has value $1$ only in $S_3$, and $h_1(1) = 2$ and $h_2(1) = 4$. Thus, we set $\text{SIG}(1, 3) = 2$ and $\text{SIG}(2, 3) = 4$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $\infty$ & $2$ & $1$ \\ 
  $h_2$ & $1$ & $\infty$ & $4$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-2}
\end{figure}

The row numbered 3 of Figure \ref{fig:matrix-hash-functions} has $1$ in $S_2$ and $S_4$ and the values of $h_1(2) = 3$ and $h_2(2) = 2$. Thus, we set $\text{SIG}(1, 2) = 3$ and $\text{SIG}(2, 2) = 2$. We could also set $\text{SIG}(1, 4) = 3$ and $\text{SIG}(2, 4) = 2$, but we do not do this because we already have a smaller value for $\text{SIG}(1, 4)$ and $\text{SIG}(2, 4)$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $2$ & $1$ \\ 
  $h_2$ & $1$ & $2$ & $4$ & $1$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

Next comes the row numbered 3 in Figure \ref{fig:matrix-hash-functions}. This row has $1$ in $S_1$, $S_3$ and $S_4$ and the values of $h_1(3) = 4$ and $h_2(3) = 0$. The value of $h_1(3)$ is greater than all values of the first row. However, the value $0$ is smaller then the values that are already present in the second row. Thus, we set $\text{SIG}(2, 1) = 0$ and $\text{SIG}(2, 3) = 0$ and $\text{SIG}(2, 4) = 0$. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $2$ & $1$ \\ 
  $h_2$ & $0$ & $3$ & $0$ & $0$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

Finally, consider the row of Figure \ref{fig:matrix-hash-functions} numbered 4. This row has $1$ only in $S_3$ and the values of $h_1(4) = 0$ and $h_2(4) = 3$. Thus, we compare $[2, 0]$ with $[0, 3]$ and set $\text{SIG}(1, 3) = 0$ and keep $\text{SIG}(2, 3) = 0$ because it is already smaller. The result is the following:

\begin{figure}[H]
\centering
\begin{tabular}{|c||c|c|c|c|}
  \hline
   & $S_1$ & $S_2$ & $S_3$ & $S_4$\\
  \hline
  $h_1$ & $1$ & $3$ & $0$ & $1$ \\ 
  $h_2$ & $0$ & $3$ & $0$ & $0$ \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
% \caption{}
\label{fig:sig-matrix-3}
\end{figure}

We can estimate the Jaccard similarities of the underlying sets from this signature matrix. Column $S_1$ and $S_4$ are identical, so $\text{Jaccard}(S_1, S_4) = 1$ but if we look at Figure \ref{fig:matrix-hash-functions}, we see that the Jaccard similarity of $S_1$ and $S_4$ is $2/3$. This is because the signature matrix is only an approximation of the Jaccard similarities, and this example is to small for the law of the large numbers to ensure that the estimate is close.

\section{Locality-Sensitive Hashing for Documents}\label{sec:lsh-documents}

Even though we can use minhashing to compress large documents into small signatures and preserve the expected similarity of any pair of documents, it still may be impossible to find the pairs with greatest similarity efficiently. The reason is that the number of pairs of documents may be too large, even if there are not too many documents. 

We need to focus our attention only on pairs that are likely to be similar, without investigating all pairs. This is the idea behind \textbf{locality-sensitive hashing} (LSH).

\subsection{LSH for Minhash Signatures}\label{sec:lsh-minhash}

One general approach to LSH it to ``hash'' items several times, doing it in such a way that if two items are similar, then the probability that they will collide is high. We can consider any pair that hashed to the same bucket as a \textit{candidate pair}.

We hope that dissimilar pairs will not collide. Notice that this is not guaranteed and we call \textit{false positives} the pairs that are dissimilar but collide. On the other hand, we call \textit{false negatives} the pairs that are similar but do not collide, and we hope that their number is small.

If we minhash signature for the items, an effective way to choose the hashing is to divide the signature matrix into $b$ bands of $r$ rows each. For each band, we hash the $r$ rows into a hash table with $n$ (big enough) buckets. We can use the same hash function for all bands but we use a different bucket array for each band, so column with the same vector in different buckets will not in the same bucket.


\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/4-signature-matrix-4-bands.tex
}
\caption{Dividing a signature matrix into four bands of three rows per band}
\label{fig:4-signature-matrix-4-bands}
\end{figure}

Figure \ref{fig:4-signature-matrix-4-bands} show part of a signature matrix of $12$ rows divided into four bands of three rows each. The second and fourth columns are identical, so they will be hashed to the same bucket in the first band and the columns that are different will be hashed to different buckets. Two columns that do not agree in Band $1$ have three other chances to become a candidate pair. However, if two columns are similar there is a good chance that they will be hashed to the same bucket in at least one of the bands.

\subsection{Analysis of the Banding Technique}\label{sec:analysis-banding-technique}

Suppose we use $b$ bands of $r$ rows, and suppose that a particular pair of documents have Jaccard similarity $s$. Recall from Section \ref{subsec:minhashing-and-jaccard-similarity} that the probability that two columns agree in a particular row is $s$. We can calculate the probability that two signature become a candidate pair as follows:

\begin{equation*}
    \begin{split}
        \mathbb{P}(\text{agree in all row of one band}) & = s^r \\
        \mathbb{P}(\text{disagree in at least one row of one band}) & = 1 - s^r \\
        \mathbb{P}(\text{disagree in at least one row of all bands}) & = (1 - s^r)^b \\    
        \mathbb{P}(\text{agree in all the rows of a band and become a candidate pair}) & = 1 - (1 - s^r)^b
    \end{split}
\end{equation*}

After choosing $r$ and $b$, this function has the form of an \textit{S-curve} as shown in Figure \ref{fig:4-s-curve}. Where in the $x$-axis we have the Jaccard similarity $s$ and in the $y$-axis we have the probability that two signatures become a candidate pair. 

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/4-s-curve.tex
}
\caption{The S-curve}
\label{fig:4-s-curve}
\end{figure}

The \textit{threshold} of similarity $s$ at which the probability of becoming a candidate pair is $1/2$ is a function of $b$ and $r$. If $b$ and $r$ are large, the threshold is high, so only pairs of documents that are very similar will be candidates. If $b$ and $r$ are small, the threshold is low, so many pairs of documents will be candidates. An approximation of the threshold is $1/b^{1/r}$.

\subsection{Combining the Techniques}\label{sec:combining-techniques}

We now give an approach to finding the set of candidate pairs for similar documents and then discovering the truly similar documents among the candidates. This approach can produce \textit{false positives} and \textit{false negatives}. 

\begin{enumerate}
    \item Pick a value $k$ and construct from each document the set of $k$-shingles. Optionally, hash the $k$-shingles to shorter bucket numbers.
    \item Sort the document-shingle pair to orger them by shingle.
    \item Pick a length $n$ for the minhash signatures. Feed the sorted list to the algorithm of Section \ref{subsec:minhash-signature} to produce the minhash signature for each document.
    \item Choose a threshold $t$ for Jaccard similarity. Pick a number of bands $b$ and a number of rows $r$ per band, such that $br = n$. If the presence of \textit{false negatives} is not acceptable, then choose $b$ and $r$ to produce a threshold lower than $t$.
    \item Construct candidate pairs by applying the LSH technique of Section \ref{sec:lsh-minhash}.
    \item Examine each candidate pair's signatures and determined if the fraction of components in which they agree is at least $t$.
    \item Optionally, if the signature are sufficiently similar, go to the documents themselves and check that they are truly similar.
\end{enumerate}

\section{Distance Measures}\label{sec:distance-measures}

The Jaccard similarity is a measure of how close sets are, it is not really a distance measure. Jaccard distance is defined as $1 - \text{Jaccard similarity}$ and it is a distance measure, but there it is not the only one.

In order to define a distance measure, we need to define a \textit{distance function} $d(x, y)$ that satisfies the following axioms:

\begin{enumerate}
    \item $d(x, y) \geq 0$ for all $x$ and $y$. (no negative distances)
    \item $d(x, y) = 0$ iif $x = y$. (distances are positive, except for identical points)
    \item $d(x, y) = d(y, x)$. (distance is simmetric)
    \item $d(x, z) \leq d(x, y) + d(y, z)$. (triangle inequality)
\end{enumerate}

\subsection{Euclidean Distance}\label{subsec:euclidean-distance}

An \textit{n-dimensional Euclidean space} is one where points are vectors of $n$ real numbers. The Euclidean distance between two points $x$ and $y$ in an $n$-dimensional Euclidean space, which we shall refer to as the $L_2$\textit{-norm}, is defined as:

\begin{equation*}
    \begin{split}
        d(x, y) & = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\\
        \text{where } x & = [x_1, x_2, \dots, x_n] \text{ and }\\ 
                      y & = [y_1, y_2, \dots, y_n]
    \end{split}
\end{equation*}

It is easy to verify the axioms of a distance function for the Euclidean distance. The Euclidean distance cannot be negative because it is the square root of a sum of squares, and the sum of square are strictly positive. On the other hand, the distance will be $0$ if $x_i = y_i$ for all $i$, which means that $x = y$. Simmetry follows because $(x_i - y_i)^2 = -(y_i - x_i)^2$. Finally, the triangle inequality follows from the fact that the square root is a concave function, so the sum of square roots is less than the square root of the sum. 

There are other distance measures that have been used for Euclidean spaces. For any constant $r$, we can define the $L_r$\textit{-norm} as:

\begin{equation*}
    \begin{split}
    d(x, y) & = \left(\sum_{i=1}^{n} \vert x_i - y_i \vert ^ r \right)^{1/r}\\
        \text{where } x & = [x_1, x_2, \dots, x_n] \text{ and }\\ 
                      y & = [y_1, y_2, \dots, y_n]
    \end{split}
\end{equation*}

The $L_1$-norm is also known as the \textit{Manhattan distance} or \textit{city-block distance}. It is the distance between two points in a city where you can only travel along the streets. The $L_\infty$-norm is the limit as $r$ approaches infinity. As $r$ gets larger, only the dimension with the largest difference between $x$ and $y$ matters. Thus, the $L_\infty$-norm is the maximum of $\vert x_i - y_i \vert$ over all $i$.

