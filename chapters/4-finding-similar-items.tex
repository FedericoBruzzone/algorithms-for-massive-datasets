\chapter{Finding Similar Items}\label{sec:finding-similar-items}

We show how the problem of finding textually similar documents can be turned into such a set problem by the technique known as ``shingling''. We introduce a technique called ``minhashing'', which compresses large sets in such a way that we can still deduce the similarity of the underlying sets from their compressed versions. Another important problem that arises when we search for similar items of any kind is the problem of ``nearest neighbor search''. We show how this problem can be solved by a technique called ``Locality Sensitive Hashing''. 

\section{Application of Near-Neighbor Search}\label{sec:application-of-near-neighbor-search}

We shall focus initially on a particular notion of ``similarity'': the similarity of sets by looking at the relative size of their intersection. This notion is called \textit{Jaccard similarity}:
\begin{equation*}
    \text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation*}
where $A$ and $B$ are sets. The Jaccard similarity is a number between 0 and 1, where 0 means that the two sets are disjoint, and 1 means that they are equal.

\subsection{Similarity of Documents}\label{subsec:similarity-of-documents}

We can use the Jaccard similarity to measure the similarity of documents. We can represent a document as a set of words that occur in it. We can then measure the similarity of two documents by measuring the similarity of the sets of words that occur in them.

In the base case, where the two documents are exact duplicates, we can compare them character by character. However in many applications, the document are not identical, but they share a lot of part of text.

\begin{itemize}
    \item Plagiarism: finding plagiarized documents tests our ability to find textually similar documents.
    \item Mirror Pages: a mirror page is a page that is identical to another page, except for some minor changes, such as the replacement of the author's name or the addition of a few words.
    \item Articles from the Same Source: we may want to find articles from the same source, such as the New York Times.
\end{itemize}

\subsection{Collaborative Filtering as a Similar-Sets Problem}\label{subsec:collaborative-filtering-as-a-similar-sets-problem}

Another class of applications where similarity of sets is very important is called \textit{collaborative filtering}, a process where we raccomend to users items that other similar users have liked. 
\\
\\
\noindent \textbf{On-line Purchases} 

\noindent We can use collaborative filtering to recommend products to users based on the products that other similar users have purchased. We can say that two users are similar if their sets of purchased products have a high Jaccard similarity. 

Collaborative filtering requires us to find the nearest neighbors of a given user. However, by combining the similarity-finding with clustering, we are able to find the nearest neighbors of a user very quickly.
\\
\\
\noindent \textbf{Movie Ratings}

\noindent We can see movies as similar if they were rented or rated highly by many of the same customers, and see customers as similar if they rented or rated highly many of the same movies. 

When out data consists of rating rather then binary decisions, we cannot use the sets of items. Some alternative approaches are:

\begin{enumerate}
    \item Ignore low-rated customer/movie pairs.
    \item Use a threshold to convert ratings to binary decisions.
    \item If ratins are on a scale of 1 to 5, we can put in a multi-set the number of times each movie was rated. In order to compute Jaccard similarity of a multi-set, we can use the following formula:
    \begin{equation*}
        \text{Jaccard}(A, B) = \frac{\sum_{i=1}^n \min(A_i, B_i)}{\sum_{i=1}^n \max(A_i, B_i)}
    \end{equation*}
\end{enumerate}


\section{Shingling of Documents}\label{sec:shingling-of-documents}

The most effective way to represent documents as sets is to construct from the document the set of short string of characters that appear within it. These strings are called \textit{shingles}. 

A \textit{k}-shingle of a document is simply a sequence of $k$ characters that appears in it. For example, suppose that $D$ is the document containing the string ``\texttt{abcdadb}'' and that $k = 2$. Then the set of 2-shingles for $D$ is $\{ \texttt{ab}, \texttt{bc}, \texttt{cd}, \texttt{da}, \texttt{db} \}$. 

\subsection{Choosing the Shingle Size}\label{subsec:choosing-the-shingle-size}

We can pick $k$ to be any constant we like. But, if $k$ is too small, then the Jaccard similarity between two documents will be very high, even if the documents are completely different. On the other hand, if $k$ is too large, then we will miss the similarity between documents that share a lot of common text.

The important thing to remember is that, $k$ should be picked large enough that the probability of a given shingle appearning in any given document is low. 

A good rule of thumb is to immagine that there are only 20 characters and estimate the number of $k$-shingles as $20^k$. For example, for an email we can choose $k = 5$, since $20^5 = 3.2$ million and an email is unlikely to contain more than a few million characters. For a web page, we can choose $k = 9$, since $20^9 = 5.4$ trillion and a web page is unlikely to contain more than a few trillion characters.

\subsection{Hashing Shingles}\label{subsec:hashing-shingles}

Instead of using substring directly as shingles, we can pick a hash function $h$ and use $h(S)$ as the shingle for any string $S$. For instance, we can pick $k = 9$ and use the hash function $h(S) = \{0, 1, \dots, 2^{32} - 1\}$, we need to store only 4 bytes for each shingle. 

Notice that we can differentiate documents better if we use 9-shingles and hash them down to four bytes than to use 4-shingles, even though the space used to represent a shingle is the same.

\subsection{Shingles Built from Words}\label{subsec:shingles-built-from-words}

An alternative form of shingle has proved effective for the problem of identifying similar news articles, as mentioned in Section \ref{subsec:similarity-of-documents}. The exploitable distinction for this problem is that the news articles are written in a rather different style than are other elements that typically appear on the page with the article. However, for the problem of finding similar news articles, it was found that defining a shingle to be a stop word followed by the next two words, regardless of whether or not they were stop words, formed a useful set of shingles. 

\section{Similarity-Preserving Summaries of Sets}\label{sec:similarity-preserving-summaries-of-sets}

Set of shingles are large. Even if we hash them to four bytes each, the set needed to store a set is still large. The goal of this section is to replace large sets by much smaller representation called ``signatures'' that preserve the similarity of the sets.

The important property we need for signature is that we can compare the signature of two sets and get a good estimate of their Jaccard similarity. It is not possible that the signature of two sets are identical but their Jaccard similarity is low.

\mcomment{book 80, pdf 98}
















