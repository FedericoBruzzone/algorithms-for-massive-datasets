\chapter{Link Analysis}\label{chap:link-analysis}

\section{Definition of PageRank}\label{sec:def-pagerank}

PageRank is a function that assigns a real number to each page in the Web. There is not one fixed algorithm for assignment of PageRank.

In general, we can define the \textit{transition matrix of the Web} to describe what happens to random surfers after one step. This matrix $M_{n \times n}$ where $n$ is the number of pages. The element $m_{ij}$ has value $1/k$ if page $j$ has $k$ arcs out, and one of them is to page $i$. Otherwise, $m_{ij} = 0$.

The probability distribution for the location of a random surfer can be described by a column vector whose $j$-th component is the probability that the surfer is at page $j$. This probability is the (idealized) PageRank function.

Suppose we start a random surfer $A$ at any of the $n$ pages of the Web with equal probability. Then the initial vector $\textbf{v}_0 = [1/n]_n$, if $M$ is the matrix of the web, the distribution of $A$ is $M^s\textbf{v}_0$ for $s = 1, 2, \ldots$ steps. To see why multiplying a distribution vector $\textbf{v}$ by $M$ gives the distribution $\textbf{x} = M\textbf{v}$ at the next step.
The probability that $A$ will be at node $i$ at the next step, is (ancient theory of \textit{Markov processes}).
\begin{equation*}
    \begin{split}
        \mathbb{P} [\textbf{x}_i]  & = \sum_{j} m_{ij}\textbf{v}_j\\
        \text{where } m_{ij} & = \mathbb{P}[A \text{ goes from } j \text{ to } i \text{ at the next step}] \\
        \textbf{v}_j & = \mathbb{P}[A \text{ is at } j \text{ at the previous step}] \\
    \end{split}
\end{equation*}

The limiting distribution $\textbf{v}$ that satisfies $\textbf{v} = M\textbf{v}$ is called the \textit{stationary distribution} of the Markov process. It is the distribution that the $A$ will eventually reach, no matter where he starts. In other words, the limiting $\textbf{v} = \lambda M \textbf{v}$ is an eigenvector of $M$ with eigenvalue $\lambda$. Remember that $M$ is a stochastic matrix, so $\sum_{j} m_{ij} = 1 \ \forall i$, and the eigenvalue associated to principal eigenvector is  $\lambda = 1$.
Recall that the intuition behind PageRank is that the more likely a surfer is to be at a page, the more important the page is.

\section{Structure of the Web}\label{sec:def-pagerank}

It would very nice if the nice if the Web were strongly connected, it is not in practice. An early study of the Web found it to have the structure shown in Fig. \ref{fig:bowtie-web}. There was a large strongly connected component (SCC), but there were several other portions that were almost as large.

\begin{enumerate}
    \item \textit{in-component}, consist of pages that can reach the SCC, but cannot be reached from the SCC.
    \item \textit{out-component}, consist of pages that can be reached from the SCC, but unable to reach the SCC.
    \item \textit{tendrils}, which are of two types. Some tendrils consist of pages reachable from the in-component but not able to reach the in-component. The other tendrils can reach the out-component, but are not reachable from the out-component. 
\end{enumerate}


\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/3-bowtie-web.tex
}
\caption{The ``bowtie'' picture of the Web}
\label{fig:bowtie-web}
\end{figure}

\begin{itemize}
    \item[(a)] \textit{Tuples}, which are pages from the in-component and able to reach the out-component, but unable to reach the SCC or be reached from the SCC.    
    \item[(b)] Isolated, components that are unreachable from the large components and unable to reach those components.
\end{itemize}

There are really two problems we need to avoid. First is the dead end, a page that has no links out. The second problem is groups of pages that all have outlinks but they never link to any other pages. 

\section{Avoiding Dead Ends}\label{sec:avoid-dead-ends}

Recall that a page with no link out is called a dead end. If we allow dead ends, the transition matrix of the web is no longer stochastic. If we compute $M^i \textbf{v}$ for increasing powers of a substoachastic matrix $M$, then some or all of the components of the vector go to $0$.

There are two approaches to dealing with dead ends.

\begin{enumerate}
    \item We can drop the dead ends from the graph, and also drop their incoming arcs. Doing so may create more dead ends, which also have to be dropped, recursively. 
    \item We can modify the process by which random surfers are assumed to move about the Web. This method, which we refer to as ``taxation'', also solves the problem of spider traps. 
\end{enumerate}

\subsection{Recursive deletion of dead ends}\label{subsec:recursive-deletion-dead-ends}

In order to compute the PageRank with dead ends, using the first method (\textbf{drop dead ends}),

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/3-graph-two-dead-ends.tex
}
\caption{A graph with two levels of dead ends}
\label{fig:graph-two-dead-ends}
\end{figure}

\begin{itemize}
    \item Drop the dead ends from the graph, and also drop their incoming arcs.  
    \item Compute the PageRank of the remaining nodes, using the method described in the previous section.
    \item Compute the PageRank of the last removed dead ends $x$ as follows:
        \begin{equation}
            \begin{split}
                PR_{dead}(x) & = \sum_{v \in G \ : \ (v, x) \in E}{PR(v) \frac{1}{\textbf{deg}(v)}} \\
                \text{where } & \textbf{deg}(v) \text{ is the out-degree of } v; \\
                              & G \text{ is the graph after removing dead ends}; \\
                              & PR(v) \text{ is the PageRank of } v \text{ in } G.
            \end{split}
        \end{equation}
        and continue recursively for all removed dead ends.
\end{itemize}

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/3-graph-reduced-dead-ends.tex
}
\caption{The reduced graph with no dead ends}
\label{fig:graph-reduced-no-dead-ends}
\end{figure}

\subsection{Spider traps and Taxation}\label{subsec:spider-traps-taxation}

A spider trap is a set of nodes with no dead ends but no arcs out. An example of a transition matrix representing the Fig. \ref{fig:graph-spider-trap} with a single spider trap is:
\begin{equation*}
    M = 
    \begin{bmatrix}
        1 & 1/2 & 0 & 0 \\
        1/3 & 0 & 0 & 1/2 \\
        1/3 & 0 & 1 & 1/2 \\
        1/3 & 1/2 & 0 & 0
    \end{bmatrix}
\end{equation*}

The problem in the following example is that the spider trap is absorbing. Once the surfer enters the spider trap, he can never leave.

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/3-graph-spider-trap.tex
}
\caption{A graph with a one-node spider trap}
\label{fig:graph-spider-trap}
\end{figure}

To avoid this problem, we can modify the calculation of PageRank by adding a small probability $\beta$ that the surfer will jump to a random page. We compute the new vector estimate of PageRanks $\textbf{v}'$ as follows: 
\begin{equation}\label{eq:taxation}
    \textbf{v}' = \beta M \textbf{v} + (1 - \beta) \textbf{e} / n
\end{equation}

where $\beta$ is a chosen constant, usually in the range $0.8$ to $0.9$, $\textbf{e}$ is a vector of all $1$'s, and $n$ is the number of nodes in the graph. The term $\beta M \textbf{v}$ represent the case where, with probability $\beta$ a random surfer will follow a link out of the current page. The term $(1 - \beta) \textbf{e} / n$ represents the case where, with probability $1 - \beta$, the surfer will jump to a random page.

\section{Efficient Computation of PageRank}\label{sec:efficient-computation-pagerank}

To compute the PageRank for a large graph representing the Web, we have to perform a matrix-vector multiplication on the order of $50$ times, until the vector is close to unchanged at one interation. There are basically two issues:
\begin{itemize}
    \item The transition matrix of the Web $M$ is very sparse. We want to take to represent the matrix by its non-zero elements.
    \item We may not be using MapReduce, or for efficient reason we may wish to use a combiner with the Map tasks to reduce the amount of data.
\end{itemize}

\subsection{Representing transition matrix}\label{subsec:representing-transition-matrix}

The proper way to represent any sparse matrix is to list the locations of the nonzero entries and their values. We use $4$-byte integers for coordinates of an element and an $8$-byte double-precision number for the value, so the total size is $16$-bytes per nonzero element. The space required is $O(n)$, where $n$ is the number of nonzero elements.

To store a transition matrix using less space, we can thus represent a column by one integer for the out-degree, and one integer per nonzero entry in that column, giving the row number where that entry is located. Thus, we need slightly more than 4 bytes per nonzero entry to represent a transition matrix.

\subsection{PageRank Iteration Using MapReduce}\label{subsec:pagerank-iteration-mapreduce}

One iteration of the PageRank algorithm involves taking an estimated PageRank vector $\textbf{v}$ and computing the next estimate $\textbf{v}'$ using Eq. \ref{eq:taxation}. 
If $n$ is small enough, we can compute $\textbf{v}'$ in memory and then write it out to a file. If $n$ is large, as it is for the Web, we can use MapReduce to compute $\textbf{v}'$ dividing $M$ into vertical stripes and $\textbf{v}$ into horizontal stripes. This method is not adequate because we are assuming that $\textbf{v}$ cannot fit in memory, and $\textbf{v}'$, that is the product between $M$ and $\textbf{v}$, has the same size as $\textbf{v}$, so it cannot fit in memory either.

An alternative strategy is based on partitioning $M$ into $k^2$ blocks, while $\textbf{v}$ are stil partitioned into $k$ stripes.

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/3-partitioned-matrix.tex
}
\caption{Partitioning a matrix into square blocks}
\label{fig:partitioned-matrix}
\end{figure}

In this method, we use $k^2$ Map tasks. Each task gets one block of $M$ denote as $M_{ij}$ and one stripe of the vecor $\textbf{v}$, which must be $\textbf{v}_j \ \forall \ k \in  i$. Note that $\textbf{v}$ is trasmitted $k$ times and each block of $M$ only once.

The advantage of this approach is that we can keep the $j$-th stripe of $\textbf{v}'$ and $\textbf{v}$ in memory while we process $M_{ij}$\\
\\
Since we are representing transition matrices in the special way described in Section \ref{subsec:representing-transition-matrix}, we need to consider how the block of Fig. \ref{fig:partitioned-matrix} is represented. For each block, we need data abould all those columns that have at least one nonzero entry within the block. If $k$ is large, then most columns will have nothing in most blocks of its stripe. That, For a given block, we not only have to list those rows that have a nonzero entry for that column, but we must repeat the out-degree for the node represented by the column. That observation upper bounds the space needed to store the blocks of a stipe at twice the space needed to store all the stipe. 

\subsection{Other Efficient Approaches to PageRank Iteration}\label{subsec:other-efficient-approaches-pagerank-iteration}

The algorithm discussed in Section \ref{subsec:pagerank-iteration-mapreduce} is not the only way to compute PageRank using MapReduce. Recall that the algorithm of Section \ref{subsec:pagerank-iteration-mapreduce} uses $k^2$ processors, assuming all Map operations are executed in parallel. We can assign all the blocks in one row of blocks to a single Map task, and thus reduce the number of Map tasks to $k$. We can read the block in a row of blocks one-at-a-time, so the matrix does not consume a significant amount of main-memory. At the same time that we read $M_{ij}$, we must read the vector stripe $\textbf{v}_j$. As a result, each of the $k$ Map tasks reads the entire vecotr $\textbf{v}$, along with $1/k$-th of the matrix.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|}
  \hline
  \textit{Source} & \textit{Degree} & \textit{Destinations} \\
  \hline
  \textit{A} & \textit{3} & \textit{B} \\
  \textit{B} & \textit{2} & \textit{A} \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\addvspace{1em}
\textrm{(a) Representation of $M_{11}$ connecting A and B to B and C}

\addvspace{2em}
\begin{tabular}{|c|c|c|}
  \hline
  \textit{Source} & \textit{Degree} & \textit{Destinations} \\
  \hline
  \textit{C} & \textit{1} & \textit{A} \\
  \textit{D} & \textit{2} & \textit{B} \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\addvspace{1em}
\textrm{(b) Representation of $M_{12}$ connecting C and D to A and B}

\addvspace{2em}
\begin{tabular}{|c|c|c|}
  \hline
  \textit{Source} & \textit{Degree} & \textit{Destinations} \\
  \hline
  \textit{A} & \textit{3} & \textit{C, D} \\
  \textit{B} & \textit{2} & \textit{D} \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\addvspace{1em}
\textrm{(c) Representation of $M_{21}$ connecting A and B to C and D}

\addvspace{2em}
\begin{tabular}{|c|c|c|}
  \hline
  \textit{Source} & \textit{Degree} & \textit{Destinations} \\
  \hline
  \textit{D} & \textit{2} & \textit{C} \\
  \hline
\end{tabular}
\captionsetup{justification=centering}\\
\addvspace{1em}
\textrm{(d) Representation of $M_{22}$ connecting C and D to C and D}

\caption{Sparse representation of the blocks of a matrix}
\label{fig:sparse-representation-blocks-matrix}
\end{figure}

The algorithm is the same as the one described in Section \ref{subsec:pagerank-iteration-mapreduce}.But the advantage of this approach is that each Map task can combine all the terms for the portion $\textbf{v}'_i$ for which it is responsible.

\section{Topic-Sensitive PageRank}\label{sec:topic-sensitive-pagerank}

We can improve the quality of the results of PageRank weighting certain pages more heavily because of their topic. In the next section, we shall see how the topic-sensitive idea can also be applied to negate the effects of a new kind of spam, called ``link spam'', that has developed to try to fool the PageRank algorithm. The \textit{topic-sensitive PageRank} approach creates one vector for each of some small number of topics, biasing the PageRank to favor pages of that topic.       

\subsection{Biased Random Walks}\label{subsec:biased-random-walks}

Suppose we have identified some pages that represent a topic such as ``sports''. To create a topic-sensitive PageRank for sports, we can arrange that the random surfers are introduced only to a random sports page, rather than to a random page of any kind. The intuition is that pages linked to by sports pages are themselves likely to be sports pages.

The mathematical formulation for the iteration that yields topic-sensitive PageRank is similar to the equation we used for general PageRank. The only difference is how we add the new surfers. Suppose $S$ is a set of integers consisting of the consisting of the row / column number for the pages we have identified as belonging to a certain topic (called the \textit{teleport set}). Let $\textbf{e}_S$ be a vector that has 1 in the components in $S$ and $0$ in other components. Then the \textit{topic-sensitive PageRank} for $S$ is the limit of the iteration
\begin{equation*}\label{eq:topic-sensitive-pagerank}
    \textbf{v}' = \beta M \textbf{v} + (1 - \beta) \frac{\textbf{e}_S}{|S|}
\end{equation*}

Hence, as usual, $M$ is the transition matrix of the Web, and $|S|$ is the size of set $S$.

\subsection{Using Topic-Sensitive PageRank}\label{subsec:using-topic-sensitive-pagerank}

In order to integrate topic-sensitive PageRank into a search engine, we must:
\begin{enumerate}
    \item Decide on the topis for which we will create specialized PageRank vectors.
    \item Pick a teleport set for each of these topics, and compute the topic-sensitive PageRank vector.
    \item Find a way of determining the set of topic that are most relevant.
    \item Use the PageRank vectors to order the results of a query.
\end{enumerate}

\noindent We have mentioned one way of selecting the topic set: use the top-level topics of the Open Directory.

The third step is probably the trickiest. Some possible approaches are:
\begin{itemize}
    \item[(a)] Allow the user to specify the topic.
    \item[(b)] Infer the topic(s) by the words that appear in the Web pages recently serched by the user. See \ref{subsec:inferring-topics-from-words}.
    \item[(c)] Infer the topic(s) by information about the user.
\end{itemize}

\subsection{Inferring Topics from Words}\label{subsec:inferring-topics-from-words}

Suffice it to say that topics are characterized by words that appear surprisingly often in documents on that topic. 

If we examine the entire Web, or a sample, we can get the frquency of each word. In making this judgment, we must be careful to avoid some extremely rare word that appears in the sports sample with relatively higher frequency.  

Once we have identified a large collection of words that appear much more frequently for all topics, we can examine other pages and classify them according to the topic that has the highest frequency of the words in the page.

Suppose that $S_1, S_2, \ldots, S_k$ are the sets of words that have been determined to be characteristic of each of the topics on our list. Let $P$ be the set of words that appear in a given page $P$. Compute the Jaccard similarity between $P$ and each of the sets $S_i$. Classify $P$ as belonging to the topic $S_i$ for which the Jaccard similarity is largest.

\section{Link Spam}\label{sec:link-spam}

The techniques for artificially increasing the PageRank of a page are collectively called \textit{link spam}.

\subsection{Architecture of a Spam Farm}\label{subsec:architecture-spam-farm}

\begin{figure}[H]
\centering
\scalebox{0.6}{
    \input figs/3-web-for-spammers.tex
}
\caption{The Web from the point of view of the link spammer}
\label{fig:web-for-spammers}
\end{figure}



A collection of pages purpose is to increas the PageRank of a certain page or pages is called a \textit{spam farm}. Fig. \ref{fig:web-for-spammers} show the simplest form of spam farm. From the point of view of the spammers, the Web is divided into three parts:

\begin{enumerate}
    \item \textit{Inaccessible pages}: most of the Web, which is not accessible to the spammers.
    \item \textit{Accessible pages}: accessible to the spammers, but not controlled by them.
    \item \textit{Own pages}: pages that are controlled by the spammers.
\end{enumerate}

The spam farm consist of the spammers's own pagas, organized in a special way as seen on the right side of Fig. \ref{fig:web-for-spammers} and some links from accessible pages to the spammer's pages. Without some links from the outside, the spam farm would be useless, since it would not even be crawled by a tipical search engine.

In the spam farm, there is a \textit{target page} $t$, at which the spammer attempts to place as much PageRank as possible. The supporting pages prevent the target page from being an isolated node.

\subsection{Analysis of a Spam Farm}\label{subsec:analysis-spam-farm}

Suppose that PageRank is computed using a taxation parameter $\beta$, that is the fraction of a page's PageRank that is distributed to the pages it links to. Let $n$ be the number of pages in the Web (including $m$ supporting pages and the target page $t$). Let $x$ (th e amount of PageRank of the accessible pages) be the sum over all accessible pages $p$ with a link to $t$ of the PageRank of $p \beta$ divided by the number of successors of $p$. Finally, let $y$ the unknown PageRank of the target page $t$.

The PageRank of each supporting page is
\begin{equation*}
    \beta \frac{y}{m} + (1 - \beta) / n
\end{equation*}

Where $\beta \frac{y}{m}$  represent the contribuion from $t$ and $(1 - \beta) / n$ is the supporting page's share of the $(1 - \beta)$ of the PageRank that is distributed uniformly to all pages.

Now, we can compute the PageRank $y$ of target page $t$. Its PageRank comes from three sources:
\begin{enumerate}
    \item Contribution $x$ from outside.
    \item $\beta$ times the PageRank of every supporting page:
    \begin{equation*}
        \beta \left(\beta \frac{y}{m} + (1 - \beta) / n\right)
    \end{equation*}
    \item $(1 - \beta)/n$ is negligible and will be dropped to simplify the analysis.
\end{enumerate}

\noindent From (1) and (2) we have:
\begin{align*}
    y & = x + \beta m \left(\beta \frac{y}{m} + \frac{(1 - \beta)}{n}\right) & \\
      & = x + \beta^2 y + \beta (1 - \beta) \frac{m}{n} & \\
      & = \frac{x}{1 - \beta^2} + \frac{\beta (1 - \beta)}{1 - \beta^2} \frac{m}{n} & \text{solving for } y \\
      & = \frac{x}{1 - \beta^2} + c \frac{m}{n} & \text{where } c = \frac{\beta}{(1 + \beta)}   
\end{align*}

\subsection{Combating Link Spam}\label{subsec:combating-link-spam}

It has become essential for search engines to detect and eliminate link spam. The first approach is to identify structure such as that shown in Fig. \ref{fig:web-for-spammers} and eliminate it. This is not easy, because the spammers can disguise the structure of their spam farm. In the second approach a search engine can modify the PageRank to lower the rank of link spam pages. 
\begin{itemize}
    \item \textit{TrustRank}: a variation of topic-sensisitve PageRank.
    \item \textit{Spam mass}, a calculation that identifies the pages that are likely to be spamm and allow the search engine to lower their PageRank or eliminate them.
\end{itemize}

\subsection{TrustRank}\label{subsec:trust-rank}

TrustRank is topic-sensitive PageRank, where the ``topic'' is a set of pages believed to be not spam. The theory is that while a spam page might easily be made to link to a trustworthy page, it is unlikely that a trustworthy page would link to a spam page.

To implement TrustRank, we need to develop a suitable teleport set of trustworthy pages. Two approaches that have been tried are:

\begin{enumerate}
    \item Let humans examine a set of pages and decide which of them are trustworthy. For example, we might pick the pages of highest PageRank to examine and it is essentially impossible to give a spam page a PageRank near the top of the list. 
    \item Pick a domain whose membership is controlled, on th assumption that is hard for a spammer to get a page in that domain. For example, we could pick the .edu domain, since university pages are unlikely to be spam farms.
\end{enumerate}

It is likely that search engines today implement a strategy of the second type routinely, so that what we think of as PageRank really is a form of TrustRank.

\subsection{Spam Mass}\label{subsec:spam-mass}

The idea behind spam mass is that we measure for each page the fraction of its PageRank that come from spam. Suppose page $p$ has PageRank $r$ and the TrustRank $t$ based on some teleport set of trustworthy pages, then the \textit{spam mass} of $p$ is:
\begin{equation*}
    \text{SM} (p) = \frac{r - t}{r}
\end{equation*}
A negative or small positive smap mass means that $p$ is probably not a spam page, while a spam mass close to 1 means that $p$ is probably a spam page.

\section{Hubs and Authorities}\label{sec:hubs-and-authorities}

The algorithm for computing ``hubs and authorities'' is a variant of PageRank. This algorithm is sometimes called \textit{HITS} (\textit{Hyperlink-Induced Topic Search}) . 

While PageRank assume a one-dimensional notion of importance for pages, HITS views important pages as having two different types of importance:

\begin{enumerate}
    \item Pages that provide information about a topic. These pages are called \textit{authorities}.
    \item Pages that provide links to authorities, where to go to find out about that topic. These pages are called \textit{hubs}.
\end{enumerate}

\noindent HITS uses a mutually recursive definition of two concepts: ``a page is a good hub if it links to good authorities, and a page is a good authority if it is linked to by good hubs.''

\subsection{Formalizing Hubbiness and Authority}\label{subsec:formalizing-hubbiness-and-authority}

We shall assign two score to each Web page. One score represents the \textit{hubbiness} of a page and the other represents the \textit{authority} of a page. Assuming that pages are enumerated, we represent these scores as two vectors $\mathbf{h}$ and $\mathbf{a}$. In order to estimate the \textit{hubbiness} and \textit{authority} of a page, we can estimate the authority of successors and the hubbiness of predecessors, respectively. Normally scale the values of the vectors $\mathbf{h}$ and a so that the largest component is 1. 

To describe the iteratie computation of $\mathbf{h}$ and $\mathbf{a}$, we need to define the \textit{adjacency/link matrix} $A$ of the Web, $L_{n \times n}$ where $n$ is the number of pages. The entry $L_{ij}$ is 1 if there is a link from page $i$ to page $j$ and 0 otherwise. Let $L^T$ be the transpose of $L$. 

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/3-graph-HITS.tex
}
\caption{Sample data for HITS}
\label{fig:HITS}
\end{figure}

\begin{figure}[H]
\begin{equation*}
\centering
    L =
    \begin{bmatrix}
        0 & 1 & 1 & 1 & 0 \\
        1 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 \\
    \end{bmatrix}
    \qquad
    L^T =
    \begin{bmatrix}
        0 & 1 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 0 & 1 & 0 \\
        1 & 1 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 \\
    \end{bmatrix}
\end{equation*}
\caption{The link matrix $L$ and its transpose $L^T$ for the graph in Fig. \ref{fig:HITS}}
\end{figure}

We can express:

\begin{align*}
    \mathbf{h} & = \lambda L \mathbf{a} & \text{ where } \lambda \text{ is the scaling factor} \\
    \mathbf{a} & = \mu L^T \mathbf{h} & \text{ where } \mu \text{ is the scaling factor}
\end{align*}

And we can compute $\mathbf{h}$ and $\mathbf{a}$ independently:

\begin{align*}
    \mathbf{h} & = \lambda \mu L L^T \mathbf{h} \\
    \mathbf{a} & = \mu \lambda L^T L \mathbf{a}
\end{align*}

Howevere, since $L L^T$ and $L^T L$ are not as sparse as $L$ and $L^T$, we can use the following algorithm:

\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\caption{HITS algorithm - \textit{Hyperlink-Induced Topic Search}}
\KwIn{A link matrix $L$}
\Begin{
    % h vector with all 1
    $\mathbf{h} \gets \mathbf{1}_n \equiv [1, 1, \dots, 1]_n$\;
    \While{$\mathbf{h}$ and $\mathbf{a}$ have not converged}{
        $\mathbf{h} \gets L^T \mathbf{a}$ \;
        $\mathbf{a} \gets L \mathbf{h}$\;
        normalize $\mathbf{h}$ and $\mathbf{a}$ \tcp*{largest component to 1}
    }
}
\KwOut{The hubbiness and authority vectors $\mathbf{h}$ and $\mathbf{a}$}
\end{algorithm}

