\chapter{MapReduce and Cost Model}\label{chap:map-reduce-cost-model}

\section{Algorithms Using MapReduce}\label{sec:algorithms-using-mapreduce}

MapReduce is not a solution to every problem, not even every problem that profitably can use many compute nodes operating in parallel. The original purpose for which the Google implementation of MapReduce was created was executed very large matrix-verctor multiplication as are needed in the calculation of PageRank (See Chapter ~\ref{chap:link-analysis}). We shall see that matrix-vector and matrix-matrix calculations fit nicely into the MapReduce style of computing. Another imortant class of operations that can use MapReduce effectively are the relational-algebra operations.   

\subsection{Matrix-Vector Multiplication by MapReduce}\label{subsec:matrix-vector-multiplication}

Suppose we have an $n \times n$ matrix $M$, whose element in row $i$ and column $j$ will be denoted $m_{ij}$. Suppose we also have a vector $\mathbf{v}$ of length $n$, whose $j$-th element is $\mathbf{v}_j$. Then the matrix-vector product is the vector $\mathbf{x}$ of length $n$, whose $i$-th element $\mathbf{v}_i$ is given by 
\begin{equation*}
    \mathbf{x}_i = \sum_{j=1}^n m_{ij} \mathbf{v}_j
\end{equation*}
Let $n$ be large, but not so large that vector $\mathbf{v}$ cannot fit in memory and thus be avaiable to every Map task.The matrix M and vector $\mathbf{v}$ are stored in the distributed file system (DFS). We assume that the row-column coordinates of each matrix element will be discoverable from from its position in the file, or because it is stored explicitly as a triple $(i, j, m_{ij})$.We also assume the position of the element $\mathbf{v}_j$ in the vector $\mathbf{v}$ is discoverable in the same way.

\textbf{The Map Function}: The Map function is written to apply to one element of $m$. Each Map task will operate on a chunk of the matrix $M$. From each matrix element $m_{ij}$ it produce a key-value pair $(i, m_{ij}\mathbf{v}_j)$

\textbf{The Reduce Function}: The Reduce function simply sumus all the values associated with a given key $i$. The result will be a pair $(i, \mathbf{x}_i)$.

\subsection{If the Vector $\mathbf{v}$ Cannot Fit in Memory}\label{subsec:vector-v-cannot-fit-in-memory}

However, it is possible that the vector $\mathbf{v}$ is so large that it will not fit in its entirety in main memory. In this case, we can divide the matrix into vertical \textit{stripes} of equal width and divide the vector into an equal number of horizontal stripes, of the same height.

\begin{figure}[H]
\centering
\scalebox{0.8}{
    \input figs/2-matrix-and-vector.tex
}
\caption{Division of a matrix and vector into stripes}
\label{fig:matrix-vector-stripes}
\end{figure}


Each Map task is assigned a chunk from one of the stripes of the matrix and gets the entire corresponding stripe of the vector. The Map and Reduce tasks can then act exactly as was described above for the case where Map tasks get the entire vector. We shall take up matrix-vector multiplication using MapReduce again in Chapter ~\ref{chap:link-analysis}.

\subsection{Relational-Algebra Operations}\label{subsec:relational-algebra-operations}

A good starting point for exploring applications of MapReduce is by considering the standard operations on relations. A \textit{relation} is a table with column headers called \textit{attributes}, rows of the relation are called \textit{tuples} and the sets of attributes of a relation is called its \textit{schema}.

We often write an expression like $R(A_1, A_2, \dots, A_n)$ to say that a relation $R$ has the attributes $(A_1, A_2, \dots, A_n)$

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|}
  \hline
  \textit{From} & \textit{To} \\
  \hline
  \texttt{url1} & \texttt{url2} \\
  \texttt{url1} & \texttt{url3} \\
  \texttt{url2} & \texttt{url3} \\
  \texttt{url3} & \texttt{url4} \\
  \dots         & \dots         \\
  \hline
\end{tabular}
\captionsetup{justification=centering}
\caption{Relation \textit{Links} consists of the set of pairs of URL`s, \\ such that the first has one or more links to the second}
\label{fig:relation-links}
\end{figure}

A relation can be stored as a file in a distributed file system. There are several standard operations on relations, often referred to as \textit{reletional algebra}, that used to implement queries. The relational-algebra operations are:

Relation (represents a table): $R(A,B) \subseteq A \times B$

\begin{enumerate}
    \setlength\itemsep{1em}

    \item \textit{Selection}: Apply a condition $C$ to each tuple in the relation and produce as output only those tuple that satisfy $C$. The result of this selection is denoted $\sigma_C(R)$.
    \begin{equation*}
        \begin{split}
        \forall t\in R  & \xrightarrow{MAP} (t,t) \text{ if } c(t) \\
               (t, (t)) & \xrightarrow{REDUCE} (t,t) 
        \end{split}
    \end{equation*}
    \textbf{Map}: For each tuple $t$ in $R$, test if it satisfies $C$. If so, produce the key-value pair $(t, t)$.

    \textbf{Reduce}: is the identity.

    \item \textit{Projection}: For some subset $S$ of the attributes of the relation, produce from each tuple only the components for the attributes in $S$. The result of this projection is denoted $\pi_S(R)$. 
    \begin{equation*}
        \begin{split}
                  \forall t\in R & \xrightarrow{MAP} (t',t') \\
            (t', (t',\dots, t')) &\xrightarrow{REDUCE} (t',t')
        \end{split}
    \end{equation*}
    \textbf{Map}: create a tuple $t'$ that contains only the attributes in $S$.

    \textbf{Reduce}: for each key $t'$ produced by the Map, there will be ore or more key-value pairs $(t', t')$, so the Reduce function will remove the duplicates.
    
    \item \textit{Union}, \textit{Intersection}, and \textit{Difference}: These operations are defined in the usual way.  
    
    \[
    \begin{aligned}
        \text{Union:}\quad &\begin{cases}
            \begin{split}
                \forall t\in R \xrightarrow{MAP} (t,t)\; &, \quad \forall t\in S \xrightarrow{MAP} (t,t) \\
                (t, (t)) &\xrightarrow{REDUCE} (t,t)\\
                (t, (t,t)) &\xrightarrow{REDUCE} (t,t)
            \end{split}
        \end{cases}\\[10pt]
        \text{Intersection:}\quad &\begin{cases}
            \begin{split}
                \forall t\in R \xrightarrow{MAP} (t,t)\; &, \quad \forall t\in S \xrightarrow{MAP} (t,t) \\
                (t, (t)) &\xrightarrow{REDUCE} \emptyset\\
                (t, (t,t)) &\xrightarrow{REDUCE} (t,t)
            \end{split}
        \end{cases}\\[10pt]
        \text{Difference:}\quad &\begin{cases}
            \begin{split}
                \forall t\in R \xrightarrow{MAP} (t, 'R')\; &, \quad \forall t\in S \xrightarrow{MAP} (t,'S') \\
                (t, R) &\xrightarrow{REDUCE} (t,t)\\
                (t, S) &\xrightarrow{REDUCE} \emptyset \\
                (t, (R,S)) &\xrightarrow{REDUCE} \emptyset
            \end{split}
        \end{cases}
    \end{aligned}
    \]
    Considering the Union:
    \begin{itemize}
        \item \textbf{Map}: Turn each input $t$ ubti a key-value pair $(t, t)$
        \item \textbf{Reduce}: Associated with each key $t$ there will be either one or two values. Produce output $(t, t)$ ub either case. 
    \end{itemize}

    Considering the Intersection: 
    \begin{itemize}
        \item \textbf{Map}: Turn each input $t$ ubti a key-value pair $(t, t)$
        \item \textbf{Reduce}: If key $t$ has value list $(t, t)$, then produce $(t, t)$, otherwise produce nothing.
    \end{itemize}

    Considering the Intersection: 
    \begin{itemize}
        \item \textbf{Map}: For a tuple $t$ in $R$, produce the key-value pair $(t', R')$. For a tuple $t$ in $S$, produce the key-value pair $(t, 'S')$.
        \item \textbf{Reduce}: For each key $t$, if the associated value list is $(t', R')$, then produce $(t, t)$, otherwise produce nothing.
    \end{itemize}

\item \textit{Natural Join} \label{item:natural-join}: Given two realtions, compare each pair of tuples, one from each relation. If the tuples agree on all the attributes that are common to the two schemas, then produce a tuple that has components for each of the attributes in either schema and agrees with the two tuples on each attribute. If the tuples disagree on one or more shared attributes, then produce nothing from this pair of tuples. All \textit{equijoins} can be executed in the same manner. The natural join of $R(A,B)$ and $S(B,C)$ is denoted $R \bowtie S$. 
    
    \begin{equation*}
        \begin{split}
                                          \forall (a,b)\in R & \xrightarrow{MAP_R} (b,(a, 'R')) \\
                                          \forall (b,c)\in S & \xrightarrow{MAP_S} (b, (c, 'S')) \\
            (b, ((a_1,R),(c_1, S), (c_3, S),(a_2, R),\dots)) & \xrightarrow{REDUCE} \begin{cases}
                                                                                        1. \text{ Sort } \ell \text{ using 2nd element} \\ 
                                                                                        2. \ a \text{ list of elements from } R \\
                                                                                        3. \ b \text{ list of elements from } S \\
                                                                                        4. \ \forall (a, c) \in a \times b \\ 
                                                                                        5. \text{ Output} (a, b, c)
                                                                                    \end{cases}
        \end{split}
    \end{equation*}

    \textbf{Map}: For each tuple $(a, b)$ of $R$, produce the key-value pair $(b, (a', R'))$. For each tuple $(b, c)$ of $S$, produce the key-value pair $(b, (c', S'))$.

    \textbf{Reduce}: Each key value $b$ will be associated with a list of pairs that are either of the form $(R, a)$ or $(S, c)$. Construct all pairs consisting of one with first component $R$ and the other with first component $S$. The output from this key and value list is a sequence of key-value pairs. Each value is one of the triple $(a, b, c)$ such that $(R, a)$ and $(S, c)$ are in the value list.

\item \textit{Grouping} and \textit{Aggregation}: Given a relation $R$, partition its tuples according to their values in one set of attributes $G$, called the \textit{grouping attributes}. The permitted aggregations are SUM, COUNT, AVG, MIN, and MAX, with the obvious meanings. We denote a grouping-and-aggregation operation on a relation $R$ by $\gamma_{X}(R)$, where $X$ is a list of element that are either.
    \begin{itemize}
        \item[(a)] A grouping attribute, or 
        \item[(b)] An expression $\theta(A)$, where $A$ is an attribute and $\theta$ is an aggregation function.
    \end{itemize}
    The result of this operation is one tuple for each group, with components for each grouping attribute and for each aggregation.

    \begin{equation*}
        \begin{split}
             \forall (a,b)\in R & \xrightarrow{MAP} (a,b)\\
            (a,(b_1,\dots,b_n)) &\xrightarrow{REDUCE} (a, \theta(b_1,\dots,b_n))
        \end{split}
    \end{equation*}

    \textbf{Map}: For each tuple $(a, b, c)$ produce the key-value pair $(a, b)$.

    \textbf{Reduce}: Each key $a$ represent a group. Apply the aggregation operator $\theta$ to the list $(b_1,\dots,b_n)$ of values associated with $a$. The output is the pair $(a, x)$, where $x$ is the result of the aggregation.

\end{enumerate}

\subsection{Matrix Multiplication}\label{subsec:matrix_multiplication}

If $M$ is a matrix with element $m_{ij}$ in row $i$ and column $j$, and $N$ is a matrix with element $n_{jk}$ in row $j$ and column $k$, then the product $P = MN$ is the matrix $P$ with element $p_{ik}$ in row $i$ and column $k$, where

\begin{equation*}
    \begin{split}
        p_{ik} = \sum_{j} m_{ij}n_{jk}
    \end{split}
\end{equation*}

Obviously, the number of columns of $M$ must be equal to the number of rows of $N$, so the sum over $j$ makes sense. We could view matrix $M$ and $N$ as relations, $M(I, J, V)$ with tuples $(i, j, m_{ij})$ and $N(J, K, V)$ with tuples $(j, k, n_{jk})$. The product $MN$ is a natural join, $M(I, J, V) \bowtie N(J, K, V)$, having only attribute $J$ in common and produce tuples $(i, j, k, v, w)$ for each tuple $(i, j, v)$ in $M$ and each tuple $(j, k, w)$ in $N$. This five-component tuple represents the pair of matrix element $(m_{ij}, n_{ik})$ but we prefer to have a four-component tuple $(i, j, k, v \times w)$ because represents the product between $m_{ij}n_{jk}$.

We can perform grouping using $I$ and $K$ as attributes and aggregation as the sum of $V \times W$, and we can implement matrix multiplication as two MapReduce operation.
Let $N_c$ the number of columns of $N$ and $M_r$ the number of rows of $M$.
\begin{equation*}
    \begin{split}
        M=[m_{ij}]_{M_r \times X} \qquad N=[n_{jk}]_{X \times N_c} & \qquad P=MN=[p_{ik}]_{M_r \times N_c}\quad p_{ik} =\sum_{j=1}^X m_{ij}n_{jk}\\
        (i, j, m_{ij}) & \in M(I,J,V)\\
        (j, k, n_{jk}) & \in N(J,K,W)\\
        M\bowtie N & (i,j,k, m_{ij}, n_{jk})
    \end{split}
\end{equation*}

\textit{First implementation}:

\begin{equation*}
    \begin{split}
                     \forall (i,j,m_{ij}) \in M & \xrightarrow{Map_M} (j, (M,i,m_{ik}))\\
                     \forall (j,k,n_{jk}) \in N & \xrightarrow{Map_N} (j, (N,k,n_{jk}))\\  
        (j, ((M,1,m_{1j}), \dots, (M,M_r,m_{M_rj})) & , (N,1,n_{j1}), \dots, (N,N_c,n_{jN_c})))
    \end{split}
\end{equation*}

\textbf{Map}: For each matrix element $m_{ij}$, produce the key value pair $(j, (M, i, m_{ij}))$ and $n_{jk}$ produce the key value pair $(j, (N, k, n_{jk}))$. $M$ and $N$ in the values are not the matrices but just labels to distinguish the two sources of tuples. 

\textbf{Reduce}: For each key $j$, look at the list of values. For each value that comes from $M$ and $N$, produce the key-value pair with key equal to $(i, k)$ and value equal to the product of these elements $m_{ij}n_{jk}$. 

Now, we perform a grouping and aggregation by another MapReduce operation.

\begin{equation*}
    \begin{split}
        ((i,j), (a_{ik} b_{kj})) & \xrightarrow{Map} ((i,j), (a_{ik} b_{kj}))\\
        ((i,j), (a_{i1} b_{1j})) & \xrightarrow{Reduce} ((i,j), p_{ij}) \text{ where } p_{ij} \in P  
    \end{split}
\end{equation*}

\textbf{Map}: This function is just the identity.  

\textbf{Reduce}: For each key $(i, k)$, producethe sum of the list of values associated with this key. The result is the key-value pair with key $(i, k)$ and value equal to the product $p_{ik}$ where $p_{ik}$ is the value of the matrix $P = MN$ in row $i$ and column $k$.
\\
\\
\textit{Second implementation}:

We can perform matrix multiplication $P = MN$ in a \textbf{single MapReduce operation}. Let $N_c$ the number of columns of $N$ and $M_r$ the number of rows of $M$. 

\begin{equation*}
    \begin{split}
        \forall m_{ij} \in M \xrightarrow{Map_M} ((i,k), (M,j,m_{ij})) \quad \forall \ k = 1, 2, \dots, N_c \\
        \forall n_{jk} \in N \xrightarrow{Map_N} ((i,k), (N,j,n_{jk})) \quad \forall \ i = 1, 2, \dots, M_r \\ 
        ((i,k), ((M, 1, m_{i1}),\dots, (M, X, m_{iX}), (N, 1, n_{1j}), \dots, (N, X, n_{Xk})) \xrightarrow{Reduce} ((i,k), p_{ik})
    \end{split}
\end{equation*}

\textbf{Map}: For each matrix element $m_{ij}$, produce the key value pair $((i, k), (M, j, m_{ij}))$ and for each matrix element $n_{jk}$, produce the key value pair $((i, k), (N, j, n_{jk}))$.

\textbf{Reduce}: Each key $(i, k)$ have an associated list of values $((M, j, m_{ij}), (N, j, n_{jk})) \ \forall \ j$. The reduce function needs to connect the two values with the same $j$, an easy way to do this step is sort by $j$ in two separate lists and the $j$-th element will have the third componenet $m_{ij}$ and $n_{jk}$, then we can compute the product $m_{ij}n_{jk}$ and sum over $j$.


\section{The communication cost model}\label{sec:communication-cost-model}

The communication cost model is a model that allows us to estimate the communication cost of a MapReduce algorithm. The communication cost is the amount of data that is transferred between the mappers and the reducers. The communication cost is the most important factor in the performance of a MapReduce algorithm. 

Imagine that an algorithm is implemented by an acyclic network of tasks. The \textit{communication cost} of a task is the size of the input to the task and the task can be measured in bytes. The \textit{communication cost of an algorithm} is the sum of the communication cost of all the tasks implementing that algorithm.

We can explain and jusity the \textbf{importance of communication} cost as follows:
\begin{itemize}
    \item The algorithm execution by each task tends to be very simple.
    \item The typical interconnect speed for a computing cluster is 1 Gbps, and it is slow compared to the speed of the CPU.
    \item Even if a task executes at a compute node that has a copy of the chunk(s) on which the task operates, that chunk normally will be stored on disk, and the time taken to move the data into main memory may exceed the time needed to operate on the data once it is available in memory.
\end{itemize}

We might still ask why we \textbf{count only input size}, and not output size. The answare to this question involves two points:
\begin{enumerate}
    \item If the output of the one task $\tau$ is the input of another task, then the size of $\tau$'s output will be accounted for when measuring the input size for the receiving task.
    \item The output is rarely large compared with the output with the input or the intermediate data produced by the algorithm. 
\end{enumerate}

An example of join operation:
\begin{equation*}
    \begin{split}
        R(A,B) \bowtie S(B,C) \\
        \forall (a,b) \in R \xrightarrow{M} (b, (a,R)) \\
        \forall (b,c) \in S \xrightarrow{M} (b, (c,S)) \\
        |R|=r, \qquad |S|=s \\
        \text{Cost: } O(r+s)
    \end{split}
\end{equation*}

\subsection{Wall-Clock Time}\label{subsec:wall-clock-time}

While communication cost often influences our choice of algorithm to use in a cluster-computing environment, we must also be aware of the importance of \textit{wall-clock time}, the time it takes a parallel algorithm to finish.

\subsection{Multiway Joins}\label{subsec:multiway-joins}

To see how analyzing the communication cost can help us choose an algorithm in the cluster-computing environment, we can consider the problem of computing a multiway join. There is a general theory in which we:
\begin{enumerate}
    \item Select certain attributes of the realtions involved in the natural \textit{multi}-join to have their value hashed, each to some number of buckets.
    \item Select the number of buckets for each of these attributes, subejct to the contraint that the product of the number for each attribute is $k$.
    \item Idetify each of the $k$ reducers with a vector of bucket numbers. These vectors have one component for each of the attributes selected in step (1).
    \item Send tuples of each realtion to all those reducers where it might find tuples to join with. The given tuple $t$ will have values for some of the attributes selected at step (1), in this way we can apply the hash function(s) to those values to determine certain components of the vector that identifies the reducers. 
\end{enumerate}
Cosider the following example $R(A, B) \bowtie S(B, C) \bowtie T(C, D)$ and suppose that $R$, $S$, and $T$ have sizes $r$, $s$, and $t$, respectively. And let $p$ be the probability that:
\begin{itemize}
    \item An $R$-tuple and an $S$-tuple have the same value for attribute $B$.
    \item An $S$-tuple and a $T$-tuple have the same value for attribute $C$.
\end{itemize}
If we join $R$ and $S$ first (see ~\ref{item:natural-join}), then the communication cost is $O(r+s)$ and the size of the intermediate join is $R \bowtie S$ is $prs$. When we join this result with $T$, the communication cost is $O(prs+t)$. The total communication cost is $O(r+s+prs+t)$. 

\begin{figure}[H]
\centering
\scalebox{1}{
    \input figs/2-3-way-join.tex
}
\caption{Sixteen reducers together perform a 3-way join}
\label{fig:3-way-join}
\end{figure}

